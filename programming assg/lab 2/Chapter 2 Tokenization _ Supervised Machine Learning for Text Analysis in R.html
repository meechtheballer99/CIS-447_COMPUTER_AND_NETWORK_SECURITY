<!DOCTYPE html>
<!-- saved from url=(0036)https://smltar.com/tokenization.html -->
<html lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 2 Tokenization | Supervised Machine Learning for Text Analysis in R</title>
  <meta name="description" content="Chapter 2 Tokenization | Supervised Machine Learning for Text Analysis in R">
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 2 Tokenization | Supervised Machine Learning for Text Analysis in R">
  <meta property="og:type" content="book">
  <meta property="og:image" content="https://smltar.com/cover.jpg">
  <meta property="og:description" content="Chapter 2 Tokenization | Supervised Machine Learning for Text Analysis in R">
  <meta name="github-repo" content="EmilHvitfeldt/smltar">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Chapter 2 Tokenization | Supervised Machine Learning for Text Analysis in R">
  
  <meta name="twitter:description" content="Chapter 2 Tokenization | Supervised Machine Learning for Text Analysis in R">
  <meta name="twitter:image" content="https://smltar.com/cover.jpg">

<meta name="author" content="Emil Hvitfeldt and Julia Silge">


<meta name="date" content="2022-05-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="https://smltar.com/language.html">
<link rel="next" href="https://smltar.com/stopwords.html">
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/jquery-3.6.0.min.js.download"></script>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/fuse.min.js.download"></script>
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/style.css" rel="stylesheet">
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-table.css" rel="stylesheet">
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-bookdown.css" rel="stylesheet">
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-highlight.css" rel="stylesheet">
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-search.css" rel="stylesheet">
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-fontsettings.css" rel="stylesheet">
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-clipboard.css" rel="stylesheet">








<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/anchor-sections.css" rel="stylesheet">
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/anchor-sections-hash.css" rel="stylesheet">
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/anchor-sections.js.download"></script>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/kePrint.js.download"></script>
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/lightable.css" rel="stylesheet">
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/htmlwidgets.js.download"></script>
<link href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plot_text_explanations.css" rel="stylesheet">
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plot_text_explanations.js.download"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/style(1).css" type="text/css">
<link rel="stylesheet" href="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/smltar.css" type="text/css">
<script type="text/javascript" src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/MathJax.js.download"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-chartest {display: block; visibility: hidden; position: absolute; top: 0; line-height: normal; font-size: 500%}
.mjx-chartest .mjx-char {display: inline}
.mjx-chartest .mjx-box {padding-top: 1000px}
.MJXc-processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MJXc-processed {display: none}
.mjx-test {display: block; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.mjx-ex-box-test {position: absolute; overflow: hidden; width: 1px; height: 60ex}
.mjx-line-box-test {display: table!important}
.mjx-line-box-test span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
#MathJax_CHTML_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.mjx-chtml .mjx-noError {line-height: 1.2; vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
.MJXc-TeX-unknown-R {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://mathjax.rstudio.com/latest/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></head>

<body><div id="MathJax_Message" style="display: none;"></div>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary"><div class="book-search" role="search"><label for="search-box" aria-hidden="false" hidden="hidden">Type to search</label><input id="search-box" type="search" class="form-control" placeholder="Type to search (Enter for navigation)" title="Use Enter or the &lt;Down&gt; key to navigate to the next match, or the &lt;Up&gt; key to the previous match"></div>
      <nav role="navigation">

<ul class="summary">
<li><a href="https://smltar.com/">Supervised Machine Learning for Text Analysis in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="https://smltar.com/index.html"><i class="fa fa-check"></i>Welcome to Supervised Machine Learning for Text Analysis in R</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="https://smltar.com/preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="https://smltar.com/preface.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="https://smltar.com/preface.html#topics-this-book-will-not-cover"><i class="fa fa-check"></i>Topics this book will not cover</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="https://smltar.com/preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="https://smltar.com/preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="https://smltar.com/preface.html#colophon"><i class="fa fa-check"></i>Colophon</a></li>
</ul></li>
<li class="part"><span><b>I Natural Language Features</b></span></li>
<li class="chapter" data-level="1" data-path="language.html"><a href="https://smltar.com/language.html"><i class="fa fa-check"></i><b>1</b> Language and modeling</a>
<ul>
<li class="chapter" data-level="1.1" data-path="language.html"><a href="https://smltar.com/language.html#linguistics-for-text-analysis" title="1.1 Linguistics for text analysis"><i class="fa fa-check"></i><b>1.1</b> Linguistics for text analysis</a></li>
<li class="chapter" data-level="1.2" data-path="language.html"><a href="https://smltar.com/language.html#morphology" title="1.2 A glimpse into one area: morphology"><i class="fa fa-check"></i><b>1.2</b> A glimpse into one area: morphology</a></li>
<li class="chapter" data-level="1.3" data-path="language.html"><a href="https://smltar.com/language.html#different-languages"><i class="fa fa-check"></i><b>1.3</b> Different languages</a></li>
<li class="chapter" data-level="1.4" data-path="language.html"><a href="https://smltar.com/language.html#other-ways-text-can-vary"><i class="fa fa-check"></i><b>1.4</b> Other ways text can vary</a></li>
<li class="chapter" data-level="1.5" data-path="language.html"><a href="https://smltar.com/language.html#languagesummary"><i class="fa fa-check"></i><b>1.5</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="1.5.1" data-path="language.html"><a href="https://smltar.com/language.html#in-this-chapter-you-learned"><i class="fa fa-check"></i><b>1.5.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html"><i class="fa fa-check"></i><b>2</b> Tokenization</a>
<ul>
<li class="chapter" data-level="2.1" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#what-is-a-token"><i class="fa fa-check"></i><b>2.1</b> What is a token?</a></li>
<li class="chapter active" data-level="2.2" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#types-of-tokens"><i class="fa fa-check"></i><b>2.2</b> Types of tokens</a>
<ul style="display: none;">
<li class="chapter" data-level="2.2.1" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#character-tokens"><i class="fa fa-check"></i><b>2.2.1</b> Character tokens</a></li>
<li class="chapter" data-level="2.2.2" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#word-tokens"><i class="fa fa-check"></i><b>2.2.2</b> Word tokens</a></li>
<li class="chapter" data-level="2.2.3" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#tokenizingngrams"><i class="fa fa-check"></i><b>2.2.3</b> Tokenizing by n-grams</a></li>
<li class="chapter" data-level="2.2.4" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#lines-sentence-and-paragraph-tokens"><i class="fa fa-check"></i><b>2.2.4</b> Lines, sentence, and paragraph tokens</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#where-does-tokenization-break-down" title="2.3 Where does tokenization break down?"><i class="fa fa-check"></i><b>2.3</b> Where does tokenization break down?</a></li>
<li class="chapter" data-level="2.4" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#building-your-own-tokenizer" title="2.4 Building your own tokenizer"><i class="fa fa-check"></i><b>2.4</b> Building your own tokenizer</a>
<ul style="display: none;">
<li class="chapter" data-level="2.4.1" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#tokenize-to-characters-only-keeping-letters"><i class="fa fa-check"></i><b>2.4.1</b> Tokenize to characters, only keeping letters</a></li>
<li class="chapter" data-level="2.4.2" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#allow-for-hyphenated-words"><i class="fa fa-check"></i><b>2.4.2</b> Allow for hyphenated words</a></li>
<li class="chapter" data-level="2.4.3" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#wrapping-it-in-a-function"><i class="fa fa-check"></i><b>2.4.3</b> Wrapping it in a function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#tokenization-for-non-latin-alphabets" title="2.5 Tokenization for non-Latin alphabets"><i class="fa fa-check"></i><b>2.5</b> Tokenization for non-Latin alphabets</a></li>
<li class="chapter" data-level="2.6" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#tokenization-benchmark"><i class="fa fa-check"></i><b>2.6</b> Tokenization benchmark</a></li>
<li class="chapter" data-level="2.7" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#tokensummary"><i class="fa fa-check"></i><b>2.7</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="2.7.1" data-path="tokenization.html"><a href="https://smltar.com/tokenization.html#in-this-chapter-you-learned-1"><i class="fa fa-check"></i><b>2.7.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stopwords.html"><a href="https://smltar.com/stopwords.html"><i class="fa fa-check"></i><b>3</b> Stop words</a>
<ul>
<li class="chapter" data-level="3.1" data-path="stopwords.html"><a href="https://smltar.com/stopwords.html#premadestopwords" title="3.1 Using premade stop word lists"><i class="fa fa-check"></i><b>3.1</b> Using premade stop word lists</a>
<ul style="display: none;">
<li class="chapter" data-level="3.1.1" data-path="stopwords.html"><a href="https://smltar.com/stopwords.html#stop-word-removal-in-r"><i class="fa fa-check"></i><b>3.1.1</b> Stop word removal in R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="stopwords.html"><a href="https://smltar.com/stopwords.html#homemadestopwords" title="3.2 Creating your own stop words list"><i class="fa fa-check"></i><b>3.2</b> Creating your own stop words list</a></li>
<li class="chapter" data-level="3.3" data-path="stopwords.html"><a href="https://smltar.com/stopwords.html#all-stop-word-lists-are-context-specific" title="3.3 All stop word lists are context-specific"><i class="fa fa-check"></i><b>3.3</b> All stop word lists are context-specific</a></li>
<li class="chapter" data-level="3.4" data-path="stopwords.html"><a href="https://smltar.com/stopwords.html#what-happens-when-you-remove-stop-words" title="3.4 What happens when you remove stop words"><i class="fa fa-check"></i><b>3.4</b> What happens when you remove stop words</a></li>
<li class="chapter" data-level="3.5" data-path="stopwords.html"><a href="https://smltar.com/stopwords.html#stop-words-in-languages-other-than-english" title="3.5 Stop words in languages other than English"><i class="fa fa-check"></i><b>3.5</b> Stop words in languages other than English</a></li>
<li class="chapter" data-level="3.6" data-path="stopwords.html"><a href="https://smltar.com/stopwords.html#stopwordssummary"><i class="fa fa-check"></i><b>3.6</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="3.6.1" data-path="stopwords.html"><a href="https://smltar.com/stopwords.html#in-this-chapter-you-learned-2"><i class="fa fa-check"></i><b>3.6.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="stemming.html"><a href="https://smltar.com/stemming.html"><i class="fa fa-check"></i><b>4</b> Stemming</a>
<ul>
<li class="chapter" data-level="4.1" data-path="stemming.html"><a href="https://smltar.com/stemming.html#how-to-stem-text-in-r"><i class="fa fa-check"></i><b>4.1</b> How to stem text in R</a></li>
<li class="chapter" data-level="4.2" data-path="stemming.html"><a href="https://smltar.com/stemming.html#should-you-use-stemming-at-all" title="4.2 Should you use stemming at all?"><i class="fa fa-check"></i><b>4.2</b> Should you use stemming at all?</a></li>
<li class="chapter" data-level="4.3" data-path="stemming.html"><a href="https://smltar.com/stemming.html#understand-a-stemming-algorithm" title="4.3 Understand a stemming algorithm"><i class="fa fa-check"></i><b>4.3</b> Understand a stemming algorithm</a></li>
<li class="chapter" data-level="4.4" data-path="stemming.html"><a href="https://smltar.com/stemming.html#handling-punctuation-when-stemming" title="4.4 Handling punctuation when stemming"><i class="fa fa-check"></i><b>4.4</b> Handling punctuation when stemming</a></li>
<li class="chapter" data-level="4.5" data-path="stemming.html"><a href="https://smltar.com/stemming.html#compare-some-stemming-options" title="4.5 Compare some stemming options"><i class="fa fa-check"></i><b>4.5</b> Compare some stemming options</a></li>
<li class="chapter" data-level="4.6" data-path="stemming.html"><a href="https://smltar.com/stemming.html#lemmatization" title="4.6 Lemmatization and stemming"><i class="fa fa-check"></i><b>4.6</b> Lemmatization and stemming</a></li>
<li class="chapter" data-level="4.7" data-path="stemming.html"><a href="https://smltar.com/stemming.html#stemming-and-stop-words"><i class="fa fa-check"></i><b>4.7</b> Stemming and stop words</a></li>
<li class="chapter" data-level="4.8" data-path="stemming.html"><a href="https://smltar.com/stemming.html#stemmingsummary"><i class="fa fa-check"></i><b>4.8</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="4.8.1" data-path="stemming.html"><a href="https://smltar.com/stemming.html#in-this-chapter-you-learned-3"><i class="fa fa-check"></i><b>4.8.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="embeddings.html"><a href="https://smltar.com/embeddings.html"><i class="fa fa-check"></i><b>5</b> Word Embeddings</a>
<ul>
<li class="chapter" data-level="5.1" data-path="embeddings.html"><a href="https://smltar.com/embeddings.html#motivatingsparse" title="5.1 Motivating embeddings for sparse, high-dimensional data"><i class="fa fa-check"></i><b>5.1</b> Motivating embeddings for sparse, high-dimensional data</a></li>
<li class="chapter" data-level="5.2" data-path="embeddings.html"><a href="https://smltar.com/embeddings.html#understand-word-embeddings-by-finding-them-yourself" title="5.2 Understand word embeddings by finding them yourself"><i class="fa fa-check"></i><b>5.2</b> Understand word embeddings by finding them yourself</a></li>
<li class="chapter" data-level="5.3" data-path="embeddings.html"><a href="https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings" title="5.3 Exploring CFPB word embeddings"><i class="fa fa-check"></i><b>5.3</b> Exploring CFPB word embeddings</a></li>
<li class="chapter" data-level="5.4" data-path="embeddings.html"><a href="https://smltar.com/embeddings.html#glove" title="5.4 Use pre-trained word embeddings"><i class="fa fa-check"></i><b>5.4</b> Use pre-trained word embeddings</a></li>
<li class="chapter" data-level="5.5" data-path="embeddings.html"><a href="https://smltar.com/embeddings.html#fairnessembeddings" title="5.5 Fairness and word embeddings"><i class="fa fa-check"></i><b>5.5</b> Fairness and word embeddings</a></li>
<li class="chapter" data-level="5.6" data-path="embeddings.html"><a href="https://smltar.com/embeddings.html#using-word-embeddings-in-the-real-world" title="5.6 Using word embeddings in the real world"><i class="fa fa-check"></i><b>5.6</b> Using word embeddings in the real world</a></li>
<li class="chapter" data-level="5.7" data-path="embeddings.html"><a href="https://smltar.com/embeddings.html#embeddingssummary"><i class="fa fa-check"></i><b>5.7</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="5.7.1" data-path="embeddings.html"><a href="https://smltar.com/embeddings.html#in-this-chapter-you-learned-4"><i class="fa fa-check"></i><b>5.7.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Machine Learning Methods</b></span></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="https://smltar.com/mloverview.html"><i class="fa fa-check"></i>Overview</a>
<ul>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="https://smltar.com/mloverview.html#should-we-even-be-doing-this" title="Should we even be doing this?"><i class="fa fa-check"></i>Should we even be doing this?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="https://smltar.com/mloverview.html#what-bias-is-already-in-the-data" title="What bias is already in the data?"><i class="fa fa-check"></i>What bias is already in the data?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="https://smltar.com/mloverview.html#can-the-code-and-data-be-audited" title="Can the code and data be audited?"><i class="fa fa-check"></i>Can the code and data be audited?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="https://smltar.com/mloverview.html#what-are-the-error-rates-for-sub-groups" title="What are the error rates for sub-groups?"><i class="fa fa-check"></i>What are the error rates for sub-groups?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="https://smltar.com/mloverview.html#what-is-the-accuracy-of-a-simple-rule-based-alternative" title="What is the accuracy of a simple rule-based alternative?"><i class="fa fa-check"></i>What is the accuracy of a simple rule-based alternative?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="https://smltar.com/mloverview.html#what-processes-are-in-place-to-handle-appeals-or-mistakes" title="What processes are in place to handle appeals or mistakes?"><i class="fa fa-check"></i>What processes are in place to handle appeals or mistakes?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="https://smltar.com/mloverview.html#how-diverse-is-the-team-that-built-it" title="How diverse is the team that built it?"><i class="fa fa-check"></i>How diverse is the team that built it?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#firstmlregression"><i class="fa fa-check"></i><b>6.1</b> A first regression model</a>
<ul style="display: none;">
<li class="chapter" data-level="6.1.1" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#firstregression"><i class="fa fa-check"></i><b>6.1.1</b> Building our first regression model</a></li>
<li class="chapter" data-level="6.1.2" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#firstregressionevaluation"><i class="fa fa-check"></i><b>6.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#regnull"><i class="fa fa-check"></i><b>6.2</b> Compare to the null model</a></li>
<li class="chapter" data-level="6.3" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#comparerf" title="6.3 Compare to a random forest model"><i class="fa fa-check"></i><b>6.3</b> Compare to a random forest model</a></li>
<li class="chapter" data-level="6.4" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#casestudystopwords" title="6.4 Case study: removing stop words"><i class="fa fa-check"></i><b>6.4</b> Case study: removing stop words</a></li>
<li class="chapter" data-level="6.5" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#casestudyngrams" title="6.5 Case study: varying n-grams"><i class="fa fa-check"></i><b>6.5</b> Case study: varying n-grams</a></li>
<li class="chapter" data-level="6.6" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#mlregressionlemmatization"><i class="fa fa-check"></i><b>6.6</b> Case study: lemmatization</a></li>
<li class="chapter" data-level="6.7" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#case-study-feature-hashing" title="6.7 Case study: feature hashing"><i class="fa fa-check"></i><b>6.7</b> Case study: feature hashing</a>
<ul style="display: none;">
<li class="chapter" data-level="6.7.1" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#text-normalization"><i class="fa fa-check"></i><b>6.7.1</b> Text normalization</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#what-evaluation-metrics-are-appropriate" title="6.8 What evaluation metrics are appropriate?"><i class="fa fa-check"></i><b>6.8</b> What evaluation metrics are appropriate?</a></li>
<li class="chapter" data-level="6.9" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#mlregressionfull"><i class="fa fa-check"></i><b>6.9</b> The full game: regression</a>
<ul style="display: none;">
<li class="chapter" data-level="6.9.1" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#preprocess-the-data"><i class="fa fa-check"></i><b>6.9.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="6.9.2" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#specify-the-model"><i class="fa fa-check"></i><b>6.9.2</b> Specify the model</a></li>
<li class="chapter" data-level="6.9.3" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#tune-the-model"><i class="fa fa-check"></i><b>6.9.3</b> Tune the model</a></li>
<li class="chapter" data-level="6.9.4" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#regression-final-evaluation"><i class="fa fa-check"></i><b>6.9.4</b> Evaluate the modeling</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#mlregressionsummary"><i class="fa fa-check"></i><b>6.10</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="6.10.1" data-path="mlregression.html"><a href="https://smltar.com/mlregression.html#in-this-chapter-you-learned-5"><i class="fa fa-check"></i><b>6.10.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html"><i class="fa fa-check"></i><b>7</b> Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#classfirstattemptlookatdata"><i class="fa fa-check"></i><b>7.1</b> A first classification model</a>
<ul style="display: none;">
<li class="chapter" data-level="7.1.1" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#classfirstmodel"><i class="fa fa-check"></i><b>7.1.1</b> Building our first classification model</a></li>
<li class="chapter" data-level="7.1.2" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#evaluation"><i class="fa fa-check"></i><b>7.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#classnull"><i class="fa fa-check"></i><b>7.2</b> Compare to the null model</a></li>
<li class="chapter" data-level="7.3" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#comparetolasso" title="7.3 Compare to a lasso classification model"><i class="fa fa-check"></i><b>7.3</b> Compare to a lasso classification model</a></li>
<li class="chapter" data-level="7.4" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#tunelasso" title="7.4 Tuning lasso hyperparameters"><i class="fa fa-check"></i><b>7.4</b> Tuning lasso hyperparameters</a></li>
<li class="chapter" data-level="7.5" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#casestudysparseencoding" title="7.5 Case study: sparse encoding"><i class="fa fa-check"></i><b>7.5</b> Case study: sparse encoding</a></li>
<li class="chapter" data-level="7.6" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#mlmulticlass"><i class="fa fa-check"></i><b>7.6</b> Two-class or multiclass?</a></li>
<li class="chapter" data-level="7.7" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#case-study-including-non-text-data" title="7.7 Case study: including non-text data"><i class="fa fa-check"></i><b>7.7</b> Case study: including non-text data</a></li>
<li class="chapter" data-level="7.8" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#case-study-data-censoring" title="7.8 Case study: data censoring"><i class="fa fa-check"></i><b>7.8</b> Case study: data censoring</a></li>
<li class="chapter" data-level="7.9" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#customfeatures" title="7.9 Case study: custom features"><i class="fa fa-check"></i><b>7.9</b> Case study: custom features</a>
<ul style="display: none;">
<li class="chapter" data-level="7.9.1" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#detect-credit-cards"><i class="fa fa-check"></i><b>7.9.1</b> Detect credit cards</a></li>
<li class="chapter" data-level="7.9.2" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#calculate-percentage-censoring"><i class="fa fa-check"></i><b>7.9.2</b> Calculate percentage censoring</a></li>
<li class="chapter" data-level="7.9.3" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#detect-monetary-amounts"><i class="fa fa-check"></i><b>7.9.3</b> Detect monetary amounts</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#what-evaluation-metrics-are-appropriate-1" title="7.10 What evaluation metrics are appropriate?"><i class="fa fa-check"></i><b>7.10</b> What evaluation metrics are appropriate?</a></li>
<li class="chapter" data-level="7.11" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#mlclassificationfull" title="7.11 The full game: classification"><i class="fa fa-check"></i><b>7.11</b> The full game: classification</a>
<ul style="display: none;">
<li class="chapter" data-level="7.11.1" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#feature-selection"><i class="fa fa-check"></i><b>7.11.1</b> Feature selection</a></li>
<li class="chapter" data-level="7.11.2" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#specify-the-model-1"><i class="fa fa-check"></i><b>7.11.2</b> Specify the model</a></li>
<li class="chapter" data-level="7.11.3" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#classification-final-evaluation"><i class="fa fa-check"></i><b>7.11.3</b> Evaluate the modeling</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#mlclassificationsummary"><i class="fa fa-check"></i><b>7.12</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="7.12.1" data-path="mlclassification.html"><a href="https://smltar.com/mlclassification.html#in-this-chapter-you-learned-6"><i class="fa fa-check"></i><b>7.12.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Deep Learning Methods</b></span></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="https://smltar.com/dloverview.html"><i class="fa fa-check"></i>Overview</a>
<ul>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="https://smltar.com/dloverview.html#spending-your-data-budget"><i class="fa fa-check"></i>Spending your data budget</a></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="https://smltar.com/dloverview.html#feature-engineering"><i class="fa fa-check"></i>Feature engineering</a></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="https://smltar.com/dloverview.html#fitting-and-tuning"><i class="fa fa-check"></i>Fitting and tuning</a></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="https://smltar.com/dloverview.html#model-evaluation"><i class="fa fa-check"></i>Model evaluation</a></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="https://smltar.com/dloverview.html#putting-the-model-process-in-context" title="Putting the model process in context"><i class="fa fa-check"></i>Putting the model process in context</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html"><i class="fa fa-check"></i><b>8</b> Dense neural networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#kickstarter"><i class="fa fa-check"></i><b>8.1</b> Kickstarter data</a></li>
<li class="chapter" data-level="8.2" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#firstdlclassification" title="8.2 A first deep learning model"><i class="fa fa-check"></i><b>8.2</b> A first deep learning model</a>
<ul style="display: none;">
<li class="chapter" data-level="8.2.1" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#dnnrecipe"><i class="fa fa-check"></i><b>8.2.1</b> Preprocessing for deep learning</a></li>
<li class="chapter" data-level="8.2.2" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#onehotsequence"><i class="fa fa-check"></i><b>8.2.2</b> One-hot sequence embedding of text</a></li>
<li class="chapter" data-level="8.2.3" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#simple-flattened-dense-network"><i class="fa fa-check"></i><b>8.2.3</b> Simple flattened dense network</a></li>
<li class="chapter" data-level="8.2.4" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#evaluate-dnn"><i class="fa fa-check"></i><b>8.2.4</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#using-bag-of-words-features" title="8.3 Using bag-of-words features"><i class="fa fa-check"></i><b>8.3</b> Using bag-of-words features</a></li>
<li class="chapter" data-level="8.4" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#using-pre-trained-word-embeddings" title="8.4 Using pre-trained word embeddings"><i class="fa fa-check"></i><b>8.4</b> Using pre-trained word embeddings</a></li>
<li class="chapter" data-level="8.5" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#dnncross" title="8.5 Cross-validation for deep learning models"><i class="fa fa-check"></i><b>8.5</b> Cross-validation for deep learning models</a></li>
<li class="chapter" data-level="8.6" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#compare-and-evaluate-dnn-models" title="8.6 Compare and evaluate DNN models"><i class="fa fa-check"></i><b>8.6</b> Compare and evaluate DNN models</a></li>
<li class="chapter" data-level="8.7" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#dllimitations" title="8.7 Limitations of deep learning"><i class="fa fa-check"></i><b>8.7</b> Limitations of deep learning</a></li>
<li class="chapter" data-level="8.8" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#dldnnsummary"><i class="fa fa-check"></i><b>8.8</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="8.8.1" data-path="dldnn.html"><a href="https://smltar.com/dldnn.html#in-this-chapter-you-learned-7"><i class="fa fa-check"></i><b>8.8.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html" title="9 Long short-term memory (LSTM) networks"><i class="fa fa-check"></i><b>9</b> Long short-term memory (LSTM) networks</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#firstlstm"><i class="fa fa-check"></i><b>9.1</b> A first LSTM model</a>
<ul style="display: none;">
<li class="chapter" data-level="9.1.1" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#building-an-lstm"><i class="fa fa-check"></i><b>9.1.1</b> Building an LSTM</a></li>
<li class="chapter" data-level="9.1.2" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#lstmevaluation"><i class="fa fa-check"></i><b>9.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#compare-to-a-recurrent-neural-network" title="9.2 Compare to a recurrent neural network"><i class="fa fa-check"></i><b>9.2</b> Compare to a recurrent neural network</a></li>
<li class="chapter" data-level="9.3" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#bilstm" title="9.3 Case study: bidirectional LSTM"><i class="fa fa-check"></i><b>9.3</b> Case study: bidirectional LSTM</a></li>
<li class="chapter" data-level="9.4" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#case-study-stacking-lstm-layers" title="9.4 Case study: stacking LSTM layers"><i class="fa fa-check"></i><b>9.4</b> Case study: stacking LSTM layers</a></li>
<li class="chapter" data-level="9.5" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#lstmpadding"><i class="fa fa-check"></i><b>9.5</b> Case study: padding</a></li>
<li class="chapter" data-level="9.6" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#case-study-training-a-regression-model" title="9.6 Case study: training a regression model"><i class="fa fa-check"></i><b>9.6</b> Case study: training a regression model</a></li>
<li class="chapter" data-level="9.7" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#case-study-vocabulary-size" title="9.7 Case study: vocabulary size"><i class="fa fa-check"></i><b>9.7</b> Case study: vocabulary size</a></li>
<li class="chapter" data-level="9.8" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#lstmfull"><i class="fa fa-check"></i><b>9.8</b> The full game: LSTM</a>
<ul style="display: none;">
<li class="chapter" data-level="9.8.1" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#lstmfullpreprocess"><i class="fa fa-check"></i><b>9.8.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="9.8.2" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#lstmfullmodel"><i class="fa fa-check"></i><b>9.8.2</b> Specify the model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#dllstmsummary"><i class="fa fa-check"></i><b>9.9</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="9.9.1" data-path="dllstm.html"><a href="https://smltar.com/dllstm.html#in-this-chapter-you-learned-8"><i class="fa fa-check"></i><b>9.9.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html" title="10 Convolutional neural networks"><i class="fa fa-check"></i><b>10</b> Convolutional neural networks</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#what-are-cnns"><i class="fa fa-check"></i><b>10.1</b> What are CNNs?</a>
<ul style="display: none;">
<li class="chapter" data-level="10.1.1" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#kernel"><i class="fa fa-check"></i><b>10.1.1</b> Kernel</a></li>
<li class="chapter" data-level="10.1.2" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#kernel-size"><i class="fa fa-check"></i><b>10.1.2</b> Kernel size</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#firstcnn"><i class="fa fa-check"></i><b>10.2</b> A first CNN model</a></li>
<li class="chapter" data-level="10.3" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#case-study-adding-more-layers" title="10.3 Case study: adding more layers"><i class="fa fa-check"></i><b>10.3</b> Case study: adding more layers</a></li>
<li class="chapter" data-level="10.4" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#case-study-byte-pair-encoding" title="10.4 Case study: byte pair encoding"><i class="fa fa-check"></i><b>10.4</b> Case study: byte pair encoding</a></li>
<li class="chapter" data-level="10.5" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#lime" title="10.5 Case study: explainability with LIME"><i class="fa fa-check"></i><b>10.5</b> Case study: explainability with LIME</a></li>
<li class="chapter" data-level="10.6" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#keras-hyperparameter" title="10.6 Case study: hyperparameter search"><i class="fa fa-check"></i><b>10.6</b> Case study: hyperparameter search</a></li>
<li class="chapter" data-level="10.7" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#cross-validation-for-evaluation" title="10.7 Cross-validation for evaluation"><i class="fa fa-check"></i><b>10.7</b> Cross-validation for evaluation</a></li>
<li class="chapter" data-level="10.8" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#cnnfull"><i class="fa fa-check"></i><b>10.8</b> The full game: CNN</a>
<ul style="display: none;">
<li class="chapter" data-level="10.8.1" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#cnnfullpreprocess"><i class="fa fa-check"></i><b>10.8.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="10.8.2" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#cnnfullmodel"><i class="fa fa-check"></i><b>10.8.2</b> Specify the model</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#dlcnnsummary"><i class="fa fa-check"></i><b>10.9</b> Summary</a>
<ul style="display: none;">
<li class="chapter" data-level="10.9.1" data-path="dlcnn.html"><a href="https://smltar.com/dlcnn.html#in-this-chapter-you-learned-9"><i class="fa fa-check"></i><b>10.9.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Conclusion</b></span></li>
<li class="chapter" data-level="" data-path="text-models-in-the-real-world.html"><a href="https://smltar.com/text-models-in-the-real-world.html" title="Text models in the real world"><i class="fa fa-check"></i>Text models in the real world</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="regexp.html"><a href="https://smltar.com/regexp.html"><i class="fa fa-check"></i><b>A</b> Regular expressions</a>
<ul>
<li class="chapter" data-level="A.1" data-path="regexp.html"><a href="https://smltar.com/regexp.html#literal-characters"><i class="fa fa-check"></i><b>A.1</b> Literal characters</a>
<ul style="display: none;">
<li class="chapter" data-level="A.1.1" data-path="regexp.html"><a href="https://smltar.com/regexp.html#meta-characters"><i class="fa fa-check"></i><b>A.1.1</b> Meta characters</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="regexp.html"><a href="https://smltar.com/regexp.html#full-stop-the-wildcard"><i class="fa fa-check"></i><b>A.2</b> Full stop, the wildcard</a></li>
<li class="chapter" data-level="A.3" data-path="regexp.html"><a href="https://smltar.com/regexp.html#character-classes"><i class="fa fa-check"></i><b>A.3</b> Character classes</a>
<ul style="display: none;">
<li class="chapter" data-level="A.3.1" data-path="regexp.html"><a href="https://smltar.com/regexp.html#shorthand-character-classes"><i class="fa fa-check"></i><b>A.3.1</b> Shorthand character classes</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="regexp.html"><a href="https://smltar.com/regexp.html#quantifiers"><i class="fa fa-check"></i><b>A.4</b> Quantifiers</a></li>
<li class="chapter" data-level="A.5" data-path="regexp.html"><a href="https://smltar.com/regexp.html#anchors"><i class="fa fa-check"></i><b>A.5</b> Anchors</a></li>
<li class="chapter" data-level="A.6" data-path="regexp.html"><a href="https://smltar.com/regexp.html#additional-resources"><i class="fa fa-check"></i><b>A.6</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixdata.html"><a href="https://smltar.com/appendixdata.html"><i class="fa fa-check"></i><b>B</b> Data</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixdata.html"><a href="https://smltar.com/appendixdata.html#hcandersen" title="B.1 Hans Christian Andersen fairy tales"><i class="fa fa-check"></i><b>B.1</b> Hans Christian Andersen fairy tales</a></li>
<li class="chapter" data-level="B.2" data-path="appendixdata.html"><a href="https://smltar.com/appendixdata.html#scotus-opinions" title="B.2 Opinions of the Supreme Court of the United States"><i class="fa fa-check"></i><b>B.2</b> Opinions of the Supreme Court of the United States</a></li>
<li class="chapter" data-level="B.3" data-path="appendixdata.html"><a href="https://smltar.com/appendixdata.html#cfpb-complaints" title="B.3 Consumer Financial Protection Bureau (CFPB) complaints"><i class="fa fa-check"></i><b>B.3</b> Consumer Financial Protection Bureau (CFPB) complaints</a></li>
<li class="chapter" data-level="B.4" data-path="appendixdata.html"><a href="https://smltar.com/appendixdata.html#kickstarter-blurbs" title="B.4 Kickstarter campaign blurbs"><i class="fa fa-check"></i><b>B.4</b> Kickstarter campaign blurbs</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendixbaseline.html"><a href="https://smltar.com/appendixbaseline.html" title="C Baseline linear classifier"><i class="fa fa-check"></i><b>C</b> Baseline linear classifier</a>
<ul>
<li class="chapter" data-level="C.1" data-path="appendixbaseline.html"><a href="https://smltar.com/appendixbaseline.html#read-in-the-data"><i class="fa fa-check"></i><b>C.1</b> Read in the data</a></li>
<li class="chapter" data-level="C.2" data-path="appendixbaseline.html"><a href="https://smltar.com/appendixbaseline.html#split-into-testtrain-and-create-resampling-folds" title="C.2 Split into test/train and create resampling folds"><i class="fa fa-check"></i><b>C.2</b> Split into test/train and create resampling folds</a></li>
<li class="chapter" data-level="C.3" data-path="appendixbaseline.html"><a href="https://smltar.com/appendixbaseline.html#recipe-for-data-preprocessing" title="C.3 Recipe for data preprocessing"><i class="fa fa-check"></i><b>C.3</b> Recipe for data preprocessing</a></li>
<li class="chapter" data-level="C.4" data-path="appendixbaseline.html"><a href="https://smltar.com/appendixbaseline.html#lasso-regularized-classification-model" title="C.4 Lasso regularized classification model"><i class="fa fa-check"></i><b>C.4</b> Lasso regularized classification model</a></li>
<li class="chapter" data-level="C.5" data-path="appendixbaseline.html"><a href="https://smltar.com/appendixbaseline.html#a-model-workflow"><i class="fa fa-check"></i><b>C.5</b> A model workflow</a></li>
<li class="chapter" data-level="C.6" data-path="appendixbaseline.html"><a href="https://smltar.com/appendixbaseline.html#tune-the-workflow"><i class="fa fa-check"></i><b>C.6</b> Tune the workflow</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="https://smltar.com/references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body fixed">
      <div class="body-inner">
        <div class="book-header fixed" role="navigation" style="background-color: rgb(255, 255, 255);">
          <a class="btn pull-left js-toolbar-action" aria-label="Toggle Sidebar" title="Toggle Sidebar" href="https://smltar.com/tokenization.html#"><i class="fa fa-align-justify"></i></a><a class="btn pull-left js-toolbar-action" aria-label="Search" title="Search" href="https://smltar.com/tokenization.html#"><i class="fa fa-search"></i></a><div class="dropdown pull-right js-toolbar-action"><a class="btn toggle-dropdown" aria-label="Share" title="Share" href="https://smltar.com/tokenization.html#"><i class="fa fa-share-alt"></i></a><div class="dropdown-menu dropdown-left"><div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div><div class="buttons"><button class="button size-5 ">Facebook</button><button class="button size-5 ">Twitter</button><button class="button size-5 ">LinkedIn</button><button class="button size-5 ">Weibo</button><button class="button size-5 ">Instapaper</button></div></div></div><a class="btn pull-right js-toolbar-action" aria-label="Facebook" title="Facebook" href="https://smltar.com/tokenization.html#"><i class="fa fa-facebook"></i></a><a class="btn pull-right js-toolbar-action" aria-label="Twitter" title="Twitter" href="https://smltar.com/tokenization.html#"><i class="fa fa-twitter"></i></a><a class="btn pull-left js-toolbar-action" aria-label="Edit" title="Edit" href="https://smltar.com/tokenization.html#"><i class="fa fa-edit"></i></a><a class="btn pull-left js-toolbar-action" aria-label="Information about the toolbar" title="Information about the toolbar" href="https://smltar.com/tokenization.html#"><i class="fa fa-info"></i></a><h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="https://smltar.com/">Supervised Machine Learning for Text Analysis in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tokenization" class="section level1" number="2">
<h1 class="hasAnchor"><span class="header-section-number">Chapter 2</span> Tokenization<a href="https://smltar.com/tokenization.html#tokenization" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>To build features for supervised machine learning from natural language, we need some way of representing raw text as numbers so we can perform computation on them. Typically, one of the first steps in this transformation from natural language to feature, or any of kind of text analysis, is <em>tokenization</em>. Knowing what tokenization and tokens are, along with the related concept of an n-gram, is important for almost any natural language processing task.</p>
<div id="what-is-a-token" class="section level2" number="2.1">
<h2 class="hasAnchor"><span class="header-section-number">2.1</span> What is a token?<a href="https://smltar.com/tokenization.html#what-is-a-token" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In R, text is typically represented with the <em>character</em> data type, similar to strings in other languages. Lets explore text from fairy tales written by Hans Christian Andersen, available in the <strong>hcandersenr</strong> package <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-R-hcandersenr" role="doc-biblioref">Hvitfeldt 2019a</a>)</span>. This package stores text as lines such as those you would read in a book; this is just one way that you may find text data in the wild and does allow us to more easily read the text when doing analysis.
If we look at the first paragraph of one story titled The Fir-Tree, we find the text of the story is in a character vector: a series of letters, spaces, and punctuation stored as a vector.</p>
<div class="rmdpackage">
<p>
The <strong>tidyverse</strong> is a collection of packages for data manipulation, exploration, and visualization.
</p>
</div>
<div class="sourceCode" id="cb1"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="https://smltar.com/tokenization.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tokenizers)</span>
<span id="cb1-2"><a href="https://smltar.com/tokenization.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="https://smltar.com/tokenization.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidytext)</span>
<span id="cb1-4"><a href="https://smltar.com/tokenization.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(hcandersenr)</span>
<span id="cb1-5"><a href="https://smltar.com/tokenization.html#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="https://smltar.com/tokenization.html#cb1-6" aria-hidden="true" tabindex="-1"></a>the_fir_tree <span class="ot">&lt;-</span> hcandersen_en <span class="sc">%&gt;%</span></span>
<span id="cb1-7"><a href="https://smltar.com/tokenization.html#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(book <span class="sc">==</span> <span class="st">"The fir tree"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb1-8"><a href="https://smltar.com/tokenization.html#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(text)</span>
<span id="cb1-9"><a href="https://smltar.com/tokenization.html#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="https://smltar.com/tokenization.html#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(the_fir_tree, <span class="dv">9</span>)</span></code></pre></div>
<pre><code>#&gt; [1] "Far down in the forest, where the warm sun and the fresh air made a
sweet"
#&gt; [2] "resting-place, grew a pretty little fir-tree; and yet it was not happy,
it"
#&gt; [3] "wished so much to be tall like its companions the pines and firs which
grew"
#&gt; [4] "around it. The sun shone, and the soft air fluttered its leaves, and
the"
#&gt; [5] "little peasant children passed by, prattling merrily, but the fir-tree
heeded"
#&gt; [6] "them not. Sometimes the children would bring a large basket of
raspberries or"
#&gt; [7] "strawberries, wreathed on a straw, and seat themselves near the
fir-tree, and"
#&gt; [8] "say, \"Is it not a pretty little tree?\" which made it feel more
unhappy than"
#&gt; [9] "before."</code></pre>
<p>The first nine lines stores the first paragraph of the story, each line consisting of a series of character symbols.
These elements dont contain any metadata or information to tell us which characters are words and which arent. Identifying these kinds of boundaries between words is where the process of tokenization comes in.</p>
<p>In tokenization, we take an input (a string) and a token type (a meaningful unit of text, such as a word) and split the input into pieces (tokens) that correspond to the type <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-Manning:2008:IIR:1394399" role="doc-biblioref">Manning, Raghavan, and Schtze 2008</a>)</span>. Figure <a href="https://smltar.com/tokenization.html#fig:tokenizationdiag">2.1</a> outlines this process.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tokenizationdiag"></span>
<img src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/tokenization-black-box.png" alt="A black box representation of a tokenizer. The text of these three example text fragments has been converted to lowercase and punctuation has been removed before the text is split." width="90%">
<p class="caption">
FIGURE 2.1: A black box representation of a tokenizer. The text of these three example text fragments has been converted to lowercase and punctuation has been removed before the text is split.
</p>
</div>
<p>Most commonly, the meaningful unit or type of token that we want to split text into units of is a <strong>word</strong>. However, it is difficult to clearly define what a word is, for many or even most languages. Many languages, such as Chinese, do not use white space between words at all. Even languages that do use white space, including English, often have particular examples that are ambiguous <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-Bender13" role="doc-biblioref">Bender 2013</a>)</span>. Romance languages like Italian and French use pronouns and negation words that may better be considered prefixes with a space, and English contractions like didnt may more accurately be considered two words with no space.</p>
<p>To understand the process of tokenization, lets start with a overly simple definition for a word: any selection of alphanumeric (letters and numbers) symbols. Lets use some regular expressions (or regex for short, see Appendix <a href="https://smltar.com/regexp.html#regexp">A</a>) with <code>strsplit()</code> to split the first two lines of The Fir-Tree by any characters that are not alphanumeric.</p>
<div class="sourceCode" id="cb3"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="https://smltar.com/tokenization.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">strsplit</span>(the_fir_tree[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], <span class="st">"[^a-zA-Z0-9]+"</span>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt;  [1] "Far"    "down"   "in"     "the"    "forest" "where"  "the"    "warm"  
#&gt;  [9] "sun"    "and"    "the"    "fresh"  "air"    "made"   "a"      "sweet" 
#&gt; 
#&gt; [[2]]
#&gt;  [1] "resting" "place"   "grew"    "a"       "pretty"  "little"  "fir"    
#&gt;  [8] "tree"    "and"     "yet"     "it"      "was"     "not"     "happy"  
#&gt; [15] "it"</code></pre>
<p>At first sight, this result looks pretty decent. However, we have lost all punctuation, which may or may not be helpful for our modeling goal, and the hero of this story (<code>"fir-tree"</code>) was split in half. Already it is clear that tokenization is going to be quite complicated. Luckily for us, a lot of work has been invested in this process, and typically it is best to use these existing tools. For example, <strong>tokenizers</strong> <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-Mullen18" role="doc-biblioref">Mullen et al. 2018</a>)</span> and <strong>spaCy</strong> <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-spacy2" role="doc-biblioref">Honnibal et al. 2020</a>)</span> implement fast, consistent tokenizers we can use. Lets demonstrate with the <strong>tokenizers</strong> package.</p>
<div class="sourceCode" id="cb5"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="https://smltar.com/tokenization.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tokenizers)</span>
<span id="cb5-2"><a href="https://smltar.com/tokenization.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenize_words</span>(the_fir_tree[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt;  [1] "far"    "down"   "in"     "the"    "forest" "where"  "the"    "warm"  
#&gt;  [9] "sun"    "and"    "the"    "fresh"  "air"    "made"   "a"      "sweet" 
#&gt; 
#&gt; [[2]]
#&gt;  [1] "resting" "place"   "grew"    "a"       "pretty"  "little"  "fir"    
#&gt;  [8] "tree"    "and"     "yet"     "it"      "was"     "not"     "happy"  
#&gt; [15] "it"</code></pre>
<p>We see sensible single-word results here; the <code>tokenize_words()</code> function uses the <strong>stringi</strong> package <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-Gagolewski19" role="doc-biblioref">Gagolewski 2020</a>)</span> and C++ under the hood, making it very fast. Word-level tokenization is done by finding word boundaries according to the specification from the International Components for Unicode (ICU). How does this <a href="https://www.unicode.org/reports/tr29/tr29-35.html#Default_Word_Boundaries">word boundary algorithm</a> work? It can be outlined as follows:</p>
<ul>
<li><p>Break at the start and end of text, unless the text is empty.</p></li>
<li><p>Do not break within CRLF (new line characters).</p></li>
<li><p>Otherwise, break before and after new lines (including CR and LF).</p></li>
<li><p>Do not break within emoji zwj sequences.</p></li>
<li><p>Keep horizontal whitespace together.</p></li>
<li><p>Ignore Format and Extend characters, except after sot, CR, LF, and new lines.</p></li>
<li><p>Do not break between most letters.</p></li>
<li><p>Do not break letters across certain punctuation.</p></li>
<li><p>Do not break within sequences of digits, or digits adjacent to letters (3a, or A3).</p></li>
<li><p>Do not break within sequences, such as 3.2 or 3,456.789.</p></li>
<li><p>Do not break between Katakana.</p></li>
<li><p>Do not break from extenders.</p></li>
<li><p>Do not break within emoji flag sequences.</p></li>
<li><p>Otherwise, break everywhere (including around ideographs).</p></li>
</ul>
<p>While we might not understand what each and every step in this algorithm is doing, we can appreciate that it is many times more sophisticated than our initial approach of splitting on non-alphanumeric characters. In most of this book, we will use the <strong>tokenizers</strong> package as a baseline tokenizer for reference. Your choice of tokenizer will influence your results, so dont be afraid to experiment with different tokenizers or, if necessary, to write your own to fit your problem.</p>
</div>
<div id="types-of-tokens" class="section level2" number="2.2">
<h2 class="hasAnchor"><span class="header-section-number">2.2</span> Types of tokens<a href="https://smltar.com/tokenization.html#types-of-tokens" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Thinking of a token as a word is a useful way to start understanding tokenization, even if it is hard to implement concretely in software. We can generalize the idea of a token beyond only a single word to other units of text. We can tokenize text at a variety of units including:</p>
<ul>
<li><p>characters,</p></li>
<li><p>words,</p></li>
<li><p>sentences,</p></li>
<li><p>lines,</p></li>
<li><p>paragraphs, and</p></li>
<li><p>n-grams</p></li>
</ul>
<p>In the following sections, we will explore how to tokenize text using the <strong>tokenizers</strong> package. These functions take a character vector as the input and return lists of character vectors as output. This same tokenization can also be done using the <strong>tidytext</strong> <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-Silge16" role="doc-biblioref">Silge and Robinson 2016</a>)</span> package, for workflows using tidy data principles where the input and output are both in a dataframe.</p>
<div class="sourceCode" id="cb7"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="https://smltar.com/tokenization.html#cb7-1" aria-hidden="true" tabindex="-1"></a>sample_vector <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Far down in the forest"</span>,</span>
<span id="cb7-2"><a href="https://smltar.com/tokenization.html#cb7-2" aria-hidden="true" tabindex="-1"></a>                   <span class="st">"grew a pretty little fir-tree"</span>)</span>
<span id="cb7-3"><a href="https://smltar.com/tokenization.html#cb7-3" aria-hidden="true" tabindex="-1"></a>sample_tibble <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">text =</span> sample_vector)</span></code></pre></div>
<div class="rmdpackage">
<p>
The <strong>tokenizers</strong> package offers fast, consistent tokenization in R for tokens such as words, letters, n-grams, lines, paragraphs, and more.
</p>
</div>
<p>The tokenization achieved by using <code>tokenize_words()</code> on <code>sample_vector</code>:</p>
<div class="sourceCode" id="cb8"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="https://smltar.com/tokenization.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenize_words</span>(sample_vector)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "far"    "down"   "in"     "the"    "forest"
#&gt; 
#&gt; [[2]]
#&gt; [1] "grew"   "a"      "pretty" "little" "fir"    "tree"</code></pre>
<p>will yield the same results as using <code>unnest_tokens()</code> on <code>sample_tibble</code>; the only difference is the data structure, and thus how we might use the result moving forward in our analysis.</p>
<div class="sourceCode" id="cb10"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="https://smltar.com/tokenization.html#cb10-1" aria-hidden="true" tabindex="-1"></a>sample_tibble <span class="sc">%&gt;%</span></span>
<span id="cb10-2"><a href="https://smltar.com/tokenization.html#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text, <span class="at">token =</span> <span class="st">"words"</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 11  1
#&gt;    word  
#&gt;    &lt;chr&gt; 
#&gt;  1 far   
#&gt;  2 down  
#&gt;  3 in    
#&gt;  4 the   
#&gt;  5 forest
#&gt;  6 grew  
#&gt;  7 a     
#&gt;  8 pretty
#&gt;  9 little
#&gt; 10 fir   
#&gt; 11 tree</code></pre>
<div class="rmdpackage">
<p>
The <strong>tidytext</strong> package provides functions to transform text to and from tidy formats, allowing us to work seamlessly with other <strong>tidyverse</strong> tools.
</p>
</div>
<p>Arguments used in <code>tokenize_words()</code> can be passed through <code>unnest_tokens()</code> using the <a href="https://adv-r.hadley.nz/functions.html#fun-dot-dot-dot">the dots</a>, <code>...</code>.</p>
<div class="sourceCode" id="cb12"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="https://smltar.com/tokenization.html#cb12-1" aria-hidden="true" tabindex="-1"></a>sample_tibble <span class="sc">%&gt;%</span></span>
<span id="cb12-2"><a href="https://smltar.com/tokenization.html#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text, <span class="at">token =</span> <span class="st">"words"</span>, <span class="at">strip_punct =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 12  1
#&gt;    word  
#&gt;    &lt;chr&gt; 
#&gt;  1 far   
#&gt;  2 down  
#&gt;  3 in    
#&gt;  4 the   
#&gt;  5 forest
#&gt;  6 grew  
#&gt;  7 a     
#&gt;  8 pretty
#&gt;  9 little
#&gt; 10 fir   
#&gt; 11 -     
#&gt; 12 tree</code></pre>
<div id="character-tokens" class="section level3" number="2.2.1">
<h3 class="hasAnchor"><span class="header-section-number">2.2.1</span> Character tokens<a href="https://smltar.com/tokenization.html#character-tokens" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Perhaps the simplest tokenization is character tokenization, which splits texts into characters. Lets use <code>tokenize_characters()</code> with its default parameters; this function has arguments to convert to lowercase and to strip all non-alphanumeric characters. These defaults will reduce the number of different tokens that are returned. The <code>tokenize_*()</code> functions by default return a list of character vectors, one character vector for each string in the input.</p>
<div class="sourceCode" id="cb14"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="https://smltar.com/tokenization.html#cb14-1" aria-hidden="true" tabindex="-1"></a>tft_token_characters <span class="ot">&lt;-</span> <span class="fu">tokenize_characters</span>(<span class="at">x =</span> the_fir_tree,</span>
<span id="cb14-2"><a href="https://smltar.com/tokenization.html#cb14-2" aria-hidden="true" tabindex="-1"></a>                                            <span class="at">lowercase =</span> <span class="cn">TRUE</span>,</span>
<span id="cb14-3"><a href="https://smltar.com/tokenization.html#cb14-3" aria-hidden="true" tabindex="-1"></a>                                            <span class="at">strip_non_alphanum =</span> <span class="cn">TRUE</span>,</span>
<span id="cb14-4"><a href="https://smltar.com/tokenization.html#cb14-4" aria-hidden="true" tabindex="-1"></a>                                            <span class="at">simplify =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>What do we see if we take a look?</p>
<div class="sourceCode" id="cb15"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="https://smltar.com/tokenization.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tft_token_characters) <span class="sc">%&gt;%</span></span>
<span id="cb15-2"><a href="https://smltar.com/tokenization.html#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">glimpse</span>()</span></code></pre></div>
<pre><code>#&gt; List of 6
#&gt;  $ : chr [1:57] "f" "a" "r" "d" ...
#&gt;  $ : chr [1:57] "r" "e" "s" "t" ...
#&gt;  $ : chr [1:61] "w" "i" "s" "h" ...
#&gt;  $ : chr [1:56] "a" "r" "o" "u" ...
#&gt;  $ : chr [1:64] "l" "i" "t" "t" ...
#&gt;  $ : chr [1:64] "t" "h" "e" "m" ...</code></pre>
<p>We dont have to stick with the defaults. We can keep the punctuation and spaces by setting <code>strip_non_alphanum = FALSE</code>, and now we see that spaces and punctuation are included in the results too.</p>
<div class="sourceCode" id="cb17"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="https://smltar.com/tokenization.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenize_characters</span>(<span class="at">x =</span> the_fir_tree,</span>
<span id="cb17-2"><a href="https://smltar.com/tokenization.html#cb17-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">strip_non_alphanum =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb17-3"><a href="https://smltar.com/tokenization.html#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>() <span class="sc">%&gt;%</span></span>
<span id="cb17-4"><a href="https://smltar.com/tokenization.html#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">glimpse</span>()</span></code></pre></div>
<pre><code>#&gt; List of 6
#&gt;  $ : chr [1:73] "f" "a" "r" " " ...
#&gt;  $ : chr [1:74] "r" "e" "s" "t" ...
#&gt;  $ : chr [1:76] "w" "i" "s" "h" ...
#&gt;  $ : chr [1:72] "a" "r" "o" "u" ...
#&gt;  $ : chr [1:77] "l" "i" "t" "t" ...
#&gt;  $ : chr [1:77] "t" "h" "e" "m" ...</code></pre>
<p>The results have more elements because the spaces and punctuation have not been removed.</p>
<p>Depending on the format you have your text data in, it might contain ligatures. Ligatures are when multiple graphemes or letters are combined as a single character The graphemes f and l are combined into , or f and f into ff. When we apply normal tokenization rules the ligatures will not be split up.</p>
<div class="sourceCode" id="cb19"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="https://smltar.com/tokenization.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenize_characters</span>(<span class="st">"owers"</span>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "" "o" "w" "e" "r" "s"</code></pre>
<p>We might want to have these ligatures separated back into separate characters, but first, we need to consider a couple of things. First, we need to consider if the presence of ligatures is a meaningful feature to the question we are trying to answer. Second, there are two main types of ligatures: stylistic and functional. Stylistic ligatures are when two characters are combined because the spacing between the characters has been deemed unpleasant. Functional ligatures like the German Eszett (also called the scharfes S, meaning sharp s) , is an official letter of the German alphabet. It is described as a long S and Z and historically has never gotten an uppercase character. This has led the typesetters to use SZ or SS as a replacement when writing a word in uppercase. Additionally,  is omitted entirely in German writing in Switzerland and is replaced with ss. Other examples include the W in the Latin alphabet (two v or two u joined together), and , , and  in the Nordic languages. Some place names for historical reasons use the old spelling aa instead of . In Section <a href="https://smltar.com/mlregression.html#text-normalization">6.7.1</a> we will discuss text normalization approaches to deal with ligatures.</p>
</div>
<div id="word-tokens" class="section level3" number="2.2.2">
<h3 class="hasAnchor"><span class="header-section-number">2.2.2</span> Word tokens<a href="https://smltar.com/tokenization.html#word-tokens" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Tokenizing at the word level is perhaps the most common and widely used tokenization. We started our discussion in this chapter with this kind of tokenization, and as we described before, this is the procedure of splitting text into words. To do this, lets use the <code>tokenize_words()</code> function.</p>
<div class="sourceCode" id="cb21"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="https://smltar.com/tokenization.html#cb21-1" aria-hidden="true" tabindex="-1"></a>tft_token_words <span class="ot">&lt;-</span> <span class="fu">tokenize_words</span>(<span class="at">x =</span> the_fir_tree,</span>
<span id="cb21-2"><a href="https://smltar.com/tokenization.html#cb21-2" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">lowercase =</span> <span class="cn">TRUE</span>,</span>
<span id="cb21-3"><a href="https://smltar.com/tokenization.html#cb21-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">stopwords =</span> <span class="cn">NULL</span>,</span>
<span id="cb21-4"><a href="https://smltar.com/tokenization.html#cb21-4" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">strip_punct =</span> <span class="cn">TRUE</span>,</span>
<span id="cb21-5"><a href="https://smltar.com/tokenization.html#cb21-5" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">strip_numeric =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>The results show us the input text split into individual words.</p>
<div class="sourceCode" id="cb22"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="https://smltar.com/tokenization.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tft_token_words) <span class="sc">%&gt;%</span></span>
<span id="cb22-2"><a href="https://smltar.com/tokenization.html#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">glimpse</span>()</span></code></pre></div>
<pre><code>#&gt; List of 6
#&gt;  $ : chr [1:16] "far" "down" "in" "the" ...
#&gt;  $ : chr [1:15] "resting" "place" "grew" "a" ...
#&gt;  $ : chr [1:15] "wished" "so" "much" "to" ...
#&gt;  $ : chr [1:14] "around" "it" "the" "sun" ...
#&gt;  $ : chr [1:12] "little" "peasant" "children" "passed" ...
#&gt;  $ : chr [1:13] "them" "not" "sometimes" "the" ...</code></pre>
<p>We have already seen <code>lowercase = TRUE</code>, and <code>strip_punct = TRUE</code> and <code>strip_numeric = FALSE</code> control whether we remove punctuation and numeric characters, respectively. We also have <code>stopwords = NULL</code>, which we will talk about in more depth in Chapter <a href="https://smltar.com/stopwords.html#stopwords">3</a>.</p>
<p>Lets create a tibble with two fairy tales, The Fir-Tree and The Little Mermaid. Then we can use <code>unnest_tokens()</code> together with some <strong>dplyr</strong> verbs to find the most commonly used words in each.</p>
<div class="sourceCode" id="cb24"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="https://smltar.com/tokenization.html#cb24-1" aria-hidden="true" tabindex="-1"></a>hcandersen_en <span class="sc">%&gt;%</span></span>
<span id="cb24-2"><a href="https://smltar.com/tokenization.html#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(book <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"The fir tree"</span>, <span class="st">"The little mermaid"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb24-3"><a href="https://smltar.com/tokenization.html#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text) <span class="sc">%&gt;%</span></span>
<span id="cb24-4"><a href="https://smltar.com/tokenization.html#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(book, word) <span class="sc">%&gt;%</span></span>
<span id="cb24-5"><a href="https://smltar.com/tokenization.html#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(book) <span class="sc">%&gt;%</span></span>
<span id="cb24-6"><a href="https://smltar.com/tokenization.html#cb24-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(n)) <span class="sc">%&gt;%</span></span>
<span id="cb24-7"><a href="https://smltar.com/tokenization.html#cb24-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 10  3
#&gt; # Groups:   book [2]
#&gt;    book               word      n
#&gt;    &lt;chr&gt;              &lt;chr&gt; &lt;int&gt;
#&gt;  1 The fir tree       the     278
#&gt;  2 The fir tree       and     161
#&gt;  3 The fir tree       tree     76
#&gt;  4 The fir tree       it       66
#&gt;  5 The fir tree       a        56
#&gt;  6 The little mermaid the     817
#&gt;  7 The little mermaid and     398
#&gt;  8 The little mermaid of      252
#&gt;  9 The little mermaid she     240
#&gt; 10 The little mermaid to      199</code></pre>
<p>The five most common words in each fairy tale are fairly uninformative, with the exception being <code>"tree"</code> in the The Fir-Tree.</p>

<div class="rmdwarning">
<p>These uninformative words are called <strong>stop words</strong> and will be explored in-depth in Chapter <a href="https://smltar.com/stopwords.html#stopwords">3</a>.</p>
</div>
</div>
<div id="tokenizingngrams" class="section level3" number="2.2.3">
<h3 class="hasAnchor"><span class="header-section-number">2.2.3</span> Tokenizing by n-grams<a href="https://smltar.com/tokenization.html#tokenizingngrams" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An n-gram (sometimes written ngram) is a term in linguistics for a contiguous sequence of <span class="math inline"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-1-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="font-size: 110%; position: relative;"><span id="MJXc-Node-1" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-2" class="mjx-mrow"><span id="MJXc-Node-3" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.287em;">n</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-1">n</script></span> items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of <span class="math inline"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-2-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="font-size: 110%; position: relative;"><span id="MJXc-Node-4" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-5" class="mjx-mrow"><span id="MJXc-Node-6" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.287em;">n</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-2">n</script></span> words. In this book, we will use n-gram to denote word n-grams unless otherwise stated.</p>
<div class="rmdnote">
<p>
We use Latin prefixes so that a 1-gram is called a unigram, a 2-gram is called a bigram, a 3-gram called a trigram, and so on.
</p>
</div>
<p>Some example n-grams are:</p>
<ul>
<li><p><strong>unigram:</strong> Hello, day, my, little</p></li>
<li><p><strong>bigram:</strong> fir tree, fresh air, to be, Robin Hood</p></li>
<li><p><strong>trigram:</strong> You and I, please let go, no time like, the little mermaid</p></li>
</ul>
<p>The benefit of using n-grams compared to words is that n-grams capture word order that would otherwise be lost. Similarly, when we use character n-grams, we can model the beginning and end of words, because a space will be located at the end of an n-gram for the end of a word and at the beginning of an n-gram of the beginning of a word.</p>
<p>To split text into word n-grams, we can use the function <code>tokenize_ngrams()</code>. It has a few more arguments, so lets go over them one by one.</p>
<div class="sourceCode" id="cb26"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="https://smltar.com/tokenization.html#cb26-1" aria-hidden="true" tabindex="-1"></a>tft_token_ngram <span class="ot">&lt;-</span> <span class="fu">tokenize_ngrams</span>(<span class="at">x =</span> the_fir_tree,</span>
<span id="cb26-2"><a href="https://smltar.com/tokenization.html#cb26-2" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">lowercase =</span> <span class="cn">TRUE</span>,</span>
<span id="cb26-3"><a href="https://smltar.com/tokenization.html#cb26-3" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">n =</span> 3L,</span>
<span id="cb26-4"><a href="https://smltar.com/tokenization.html#cb26-4" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">n_min =</span> 3L,</span>
<span id="cb26-5"><a href="https://smltar.com/tokenization.html#cb26-5" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">stopwords =</span> <span class="fu">character</span>(),</span>
<span id="cb26-6"><a href="https://smltar.com/tokenization.html#cb26-6" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">ngram_delim =</span> <span class="st">" "</span>,</span>
<span id="cb26-7"><a href="https://smltar.com/tokenization.html#cb26-7" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">simplify =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>We have seen the arguments <code>lowercase</code>, <code>stopwords</code>, and <code>simplify</code> before; they work the same as for the other tokenizers. We also have <code>n</code>, the argument to determine which degree of n-gram to return. Using <code>n = 1</code> returns unigrams, <code>n = 2</code> bigrams, <code>n = 3</code> gives trigrams, and so on. Related to <code>n</code> is the <code>n_min</code> argument, which specifies the minimum number of n-grams to include. By default both <code>n</code> and <code>n_min</code> are set to 3 making <code>tokenize_ngrams()</code> return only trigrams. By setting <code>n = 3</code> and <code>n_min = 1</code>, we will get all unigrams, bigrams, and trigrams of a text. Lastly, we have the <code>ngram_delim</code> argument, which specifies the separator between words in the n-grams; notice that this defaults to a space.</p>
<p>Lets look at the result of n-gram tokenization for the first line of The Fir-Tree.</p>
<div class="sourceCode" id="cb27"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="https://smltar.com/tokenization.html#cb27-1" aria-hidden="true" tabindex="-1"></a>tft_token_ngram[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>#&gt;  [1] "far down in"      "down in the"      "in the forest"    "the forest where"
#&gt;  [5] "forest where the" "where the warm"   "the warm sun"     "warm sun and"    
#&gt;  [9] "sun and the"      "and the fresh"    "the fresh air"    "fresh air made"  
#&gt; [13] "air made a"       "made a sweet"</code></pre>
<p>Notice how the words in the trigrams overlap so that the word down appears in the middle of the first trigram and beginning of the second trigram. N-gram tokenization slides along the text to create overlapping sets of tokens.</p>
<p>It is important to choose the right value for <code>n</code> when using n-grams for the question we want to answer. Using unigrams is faster and more efficient, but we dont capture information about word order. Using a higher value for <code>n</code> keeps more information, but the vector space of tokens increases dramatically, corresponding to a reduction in token counts. A sensible starting point in most cases is three. However, if you dont have a large vocabulary in your data set, consider starting at two instead of three and experimenting from there. Figure <a href="https://smltar.com/tokenization.html#fig:ngramtokens">2.2</a> demonstrates how token frequency starts to decrease dramatically for trigrams and higher-order n-grams.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ngramtokens"></span>
<img src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/ngramtokens-1.svg" alt="Using longer n-grams results in a higher number of unique tokens with fewer counts. Note that the color maps to counts on a logarithmic scale." width="672">
<p class="caption">
FIGURE 2.2: Using longer n-grams results in a higher number of unique tokens with fewer counts. Note that the color maps to counts on a logarithmic scale.
</p>
</div>
<p>We are not limited to use only one degree of n-grams. We can, for example, combine unigrams and bigrams in an analysis or model. Getting multiple degrees of n-grams is a little different depending on what package you are using; using <code>tokenize_ngrams()</code> you can specify <code>n</code> and <code>n_min</code>.</p>
<div class="sourceCode" id="cb29"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="https://smltar.com/tokenization.html#cb29-1" aria-hidden="true" tabindex="-1"></a>tft_token_ngram <span class="ot">&lt;-</span> <span class="fu">tokenize_ngrams</span>(<span class="at">x =</span> the_fir_tree,</span>
<span id="cb29-2"><a href="https://smltar.com/tokenization.html#cb29-2" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">n =</span> 2L,</span>
<span id="cb29-3"><a href="https://smltar.com/tokenization.html#cb29-3" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">n_min =</span> 1L)</span>
<span id="cb29-4"><a href="https://smltar.com/tokenization.html#cb29-4" aria-hidden="true" tabindex="-1"></a>tft_token_ngram[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>#&gt;  [1] "far"          "far down"     "down"         "down in"      "in"          
#&gt;  [6] "in the"       "the"          "the forest"   "forest"       "forest where"
#&gt; [11] "where"        "where the"    "the"          "the warm"     "warm"        
#&gt; [16] "warm sun"     "sun"          "sun and"      "and"          "and the"     
#&gt; [21] "the"          "the fresh"    "fresh"        "fresh air"    "air"         
#&gt; [26] "air made"     "made"         "made a"       "a"            "a sweet"     
#&gt; [31] "sweet"</code></pre>
<p>Combining different degrees of n-grams can allow you to extract different levels of detail from text data. Unigrams tell you which individual words have been used a lot of times; some of these words could be overlooked in bigram or trigram counts if they dont co-appear with other words often. Consider a scenario where every time the word dog was used it came after an adjective: happy dog, sad dog, brown dog, white dog, playful dog, etc. If this is fairly consistent and the adjectives varied enough, then bigrams would not be able to detect that this story is about dogs. Similarly very happy and not happy will be recognized as different from bigrams and not with unigrams alone.</p>
</div>
<div id="lines-sentence-and-paragraph-tokens" class="section level3" number="2.2.4">
<h3 class="hasAnchor"><span class="header-section-number">2.2.4</span> Lines, sentence, and paragraph tokens<a href="https://smltar.com/tokenization.html#lines-sentence-and-paragraph-tokens" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Tokenizers to split text into larger units of text like lines, sentences, and paragraphs are rarely used directly for modeling purposes, as the tokens produced tend to be fairly unique. It is very uncommon for multiple sentences in a text to be identical! However, these tokenizers are useful for preprocessing and labeling.</p>
<p>For example, Jane Austens novel <em>Northanger Abbey</em> (as available in the <strong>janeaustenr</strong> package) is already preprocessed with each line being at most 80 characters long. However, it might be useful to split the data into chapters and paragraphs instead.</p>
<p>Lets create a function that takes a dataframe containing a variable called <code>text</code> and turns it into a dataframe where the text is transformed into paragraphs. First, we can collapse the text into one long string using <code>collapse = "\n"</code> to denote line breaks, and then next we can use <code>tokenize_paragraphs()</code> to identify the paragraphs and put them back into a dataframe. We can add a paragraph count with <code>row_number()</code>.</p>
<div class="sourceCode" id="cb31"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="https://smltar.com/tokenization.html#cb31-1" aria-hidden="true" tabindex="-1"></a>add_paragraphs <span class="ot">&lt;-</span> <span class="cf">function</span>(data) {</span>
<span id="cb31-2"><a href="https://smltar.com/tokenization.html#cb31-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(data, text) <span class="sc">%&gt;%</span></span>
<span id="cb31-3"><a href="https://smltar.com/tokenization.html#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(<span class="at">collapse =</span> <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb31-4"><a href="https://smltar.com/tokenization.html#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tokenize_paragraphs</span>() <span class="sc">%&gt;%</span></span>
<span id="cb31-5"><a href="https://smltar.com/tokenization.html#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">unlist</span>() <span class="sc">%&gt;%</span></span>
<span id="cb31-6"><a href="https://smltar.com/tokenization.html#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">tibble</span>(<span class="at">text =</span> .) <span class="sc">%&gt;%</span></span>
<span id="cb31-7"><a href="https://smltar.com/tokenization.html#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">paragraph =</span> <span class="fu">row_number</span>())</span>
<span id="cb31-8"><a href="https://smltar.com/tokenization.html#cb31-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Now we take the raw text data and add the chapter count by detecting when the characters <code>"CHAPTER"</code> appears at the beginning of a line. Then we <code>nest()</code> the text column, apply our <code>add_paragraphs()</code> function, and then <code>unnest()</code> again.</p>
<div class="sourceCode" id="cb32"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="https://smltar.com/tokenization.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(janeaustenr)</span>
<span id="cb32-2"><a href="https://smltar.com/tokenization.html#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="https://smltar.com/tokenization.html#cb32-3" aria-hidden="true" tabindex="-1"></a>northangerabbey_paragraphed <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">text =</span> northangerabbey) <span class="sc">%&gt;%</span></span>
<span id="cb32-4"><a href="https://smltar.com/tokenization.html#cb32-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">chapter =</span> <span class="fu">cumsum</span>(<span class="fu">str_detect</span>(text, <span class="st">"^CHAPTER "</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb32-5"><a href="https://smltar.com/tokenization.html#cb32-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(chapter <span class="sc">&gt;</span> <span class="dv">0</span>,</span>
<span id="cb32-6"><a href="https://smltar.com/tokenization.html#cb32-6" aria-hidden="true" tabindex="-1"></a>         <span class="sc">!</span><span class="fu">str_detect</span>(text, <span class="st">"^CHAPTER "</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb32-7"><a href="https://smltar.com/tokenization.html#cb32-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">data =</span> text) <span class="sc">%&gt;%</span></span>
<span id="cb32-8"><a href="https://smltar.com/tokenization.html#cb32-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">data =</span> <span class="fu">map</span>(data, add_paragraphs)) <span class="sc">%&gt;%</span></span>
<span id="cb32-9"><a href="https://smltar.com/tokenization.html#cb32-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(<span class="at">cols =</span> <span class="fu">c</span>(data))</span>
<span id="cb32-10"><a href="https://smltar.com/tokenization.html#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="https://smltar.com/tokenization.html#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(northangerabbey_paragraphed)</span></code></pre></div>
<pre><code>#&gt; Rows: 1,020
#&gt; Columns: 3
#&gt; $ chapter   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 
#&gt; $ text      &lt;chr&gt; "No one who had ever seen Catherine Morland in her infancy w
#&gt; $ paragraph &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1</code></pre>
<p>Now we have 1020 separate paragraphs we can analyze. Similarly, we could go a step further to split these chapters into sentences, lines, or words.</p>
<p>It can be useful to be able to reshape text data to get a different observational unit. As an example, if you wanted to build a sentiment classifier that would classify sentences as hostile or not, then you need to work with and train your model on sentences of text. Turning pages or paragraphs into sentences is a necessary step in your workflow.</p>
<p>Let us look at how we can turn <code>the_fir_tree</code> from a one line per element vector to a one sentence per element. <code>the_fir_tree</code> comes as a vector so we start by using <code>paste()</code> to combine the lines back together. We use a space as the separator, and then we pass it to the <code>tokenize_sentences()</code> function from the <strong>tokenizers</strong> package, which will perform sentence splitting.</p>
<div class="sourceCode" id="cb34"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="https://smltar.com/tokenization.html#cb34-1" aria-hidden="true" tabindex="-1"></a>the_fir_tree_sentences <span class="ot">&lt;-</span> the_fir_tree <span class="sc">%&gt;%</span></span>
<span id="cb34-2"><a href="https://smltar.com/tokenization.html#cb34-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">paste</span>(<span class="at">collapse =</span> <span class="st">" "</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-3"><a href="https://smltar.com/tokenization.html#cb34-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tokenize_sentences</span>()</span>
<span id="cb34-4"><a href="https://smltar.com/tokenization.html#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="https://smltar.com/tokenization.html#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(the_fir_tree_sentences[[<span class="dv">1</span>]])</span></code></pre></div>
<pre><code>#&gt; [1] "Far down in the forest, where the warm sun and the fresh air made a
sweet resting-place, grew a pretty little fir-tree; and yet it was not happy,
it wished so much to be tall like its companions the pines and firs which grew
around it."
#&gt; [2] "The sun shone, and the soft air fluttered its leaves, and the little
peasant children passed by, prattling merrily, but the fir-tree heeded them
not."
#&gt; [3] "Sometimes the children would bring a large basket of raspberries or
strawberries, wreathed on a straw, and seat themselves near the fir-tree, and
say, \"Is it not a pretty little tree?\""
#&gt; [4] "which made it feel more unhappy than before."
#&gt; [5] "And yet all this while the tree grew a notch or joint taller every
year; for by the number of joints in the stem of a fir-tree we can discover its
age."
#&gt; [6] "Still, as it grew, it complained."</code></pre>
<p>If you have lines from different categories as we have in the <code>hcandersen_en</code> dataframe, which contains all the lines of the fairy tales in English, then we would like to be able to turn these lines into sentences while preserving the <code>book</code> column in the data set.
To do this we use <code>nest()</code> and <code>map_chr()</code> to create a dataframe where each fairy tale is its own element and then we use the <code>unnest_sentences()</code> function from the tidytext package to split the text into sentences.</p>
<div class="sourceCode" id="cb36"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="https://smltar.com/tokenization.html#cb36-1" aria-hidden="true" tabindex="-1"></a>hcandersen_sentences <span class="ot">&lt;-</span> hcandersen_en <span class="sc">%&gt;%</span></span>
<span id="cb36-2"><a href="https://smltar.com/tokenization.html#cb36-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">data =</span> <span class="fu">c</span>(text)) <span class="sc">%&gt;%</span></span>
<span id="cb36-3"><a href="https://smltar.com/tokenization.html#cb36-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">data =</span> <span class="fu">map_chr</span>(data, <span class="sc">~</span> <span class="fu">paste</span>(.x<span class="sc">$</span>text, <span class="at">collapse =</span> <span class="st">" "</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb36-4"><a href="https://smltar.com/tokenization.html#cb36-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_sentences</span>(sentences, data)</span></code></pre></div>
<p>Now that we have turned the text into one sentence per element, we can analyze on the sentence level.</p>
</div>
</div>
<div id="where-does-tokenization-break-down" class="section level2" number="2.3">
<h2 class="hasAnchor"><span class="header-section-number">2.3</span> Where does tokenization break down?<a href="https://smltar.com/tokenization.html#where-does-tokenization-break-down" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Tokenization will generally be one of the first steps when building a model or any kind of text analysis, so it is important to consider carefully what happens in this step of data preprocessing. As with most software, there is a trade-off between speed and customizability, as demonstrated in Section <a href="https://smltar.com/tokenization.html#tokenization-benchmark">2.6</a>. The fastest tokenization methods give us less control over how it is done.</p>
<p>While the defaults work well in many cases, we encounter situations where we want to impose stricter rules to get better or different tokenized results. Consider the following sentence.</p>
<blockquote>
<p>Dont forget you owe the bank $1 million for the house.</p>
</blockquote>
<p>This sentence has several interesting aspects that we need to decide whether to keep or to ignore when tokenizing. The first issue is the contraction in <code>"Don't"</code>, which presents us with several possible options. The fastest option is to keep this as one word, but it could also be split up into <code>"do"</code> and <code>"n't"</code>.</p>
<p>The next issue at hand is how to deal with <code>"$1"</code>; the dollar sign is an important part of this sentence as it denotes a kind of currency. We could either remove or keep this punctuation symbol, and if we keep the dollar sign, we can choose between keeping one or two tokens, <code>"$1"</code> or <code>"$"</code> and <code>"1"</code>. If we look at the default for <code>tokenize_words()</code>, we notice that it defaults to removing most punctuation including $.</p>
<div class="sourceCode" id="cb37"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="https://smltar.com/tokenization.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenize_words</span>(<span class="st">"$1"</span>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "1"</code></pre>
<p>We can keep the dollar sign if we dont strip punctuation.</p>
<div class="sourceCode" id="cb39"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="https://smltar.com/tokenization.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenize_words</span>(<span class="st">"$1"</span>, <span class="at">strip_punct =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "$" "1"</code></pre>
<p>When dealing with this sentence, we also need to decide whether to keep the final period as a token or not. If we remove it, we will not be able to locate the last word in a sentence using n-grams.</p>
<p>Information lost to tokenization (especially default tokenization) occurs more frequently in online and more casual text. Multiple spaces, extreme use of exclamation characters, and deliberate use of capitalization can be completely lost depending on our choice of tokenizer and tokenization parameters. At the same time, it is not always worth keeping that kind of information about how text is being used. If we are studying trends in disease epidemics using Twitter data, the style the tweets are written in is likely not nearly as important as what words are used. However, if we are trying to model social groupings, language style and how individuals use language toward each other becomes much more important.</p>
<p>Another thing to consider is the degree of compression each type of tokenization provides. The choice of tokenization results in a different pool of possible tokens and can influence performance. By choosing a method that gives fewer possible tokens you allow later computational tasks to be performed faster. However, that comes with the risk of collapsing together categories of a different meaning. It is also worth noting that the spread of the number of different tokens varies with your choice of tokenizer.</p>
<p>Figure <a href="https://smltar.com/tokenization.html#fig:tokendists">2.3</a> illustrates these points. Each of the fairy tales from <strong>hcandersenr</strong> has been tokenized in five different ways and the number of distinct tokens has been plotted along the x-axis (note that the x-axis is logarithmic). We see that the number of distinct tokens decreases if we convert words to lowercase or extract word stems (see Chapter <a href="https://smltar.com/stemming.html#stemming">4</a> for more on stemming). Second, notice that the distributions of distinct tokens for character tokenizers are quite narrow; these texts use all or most of the letters in the English alphabet.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tokendists"></span>
<img src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/tokendists-1.svg" alt="The number of distinct tokens can vary enormously for different tokenizers" width="672">
<p class="caption">
FIGURE 2.3: The number of distinct tokens can vary enormously for different tokenizers
</p>
</div>
</div>
<div id="building-your-own-tokenizer" class="section level2" number="2.4">
<h2 class="hasAnchor"><span class="header-section-number">2.4</span> Building your own tokenizer<a href="https://smltar.com/tokenization.html#building-your-own-tokenizer" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sometimes the out-of-the-box tokenizers wont be able to do what you need them to do. In this case, we will have to wield <strong>stringi</strong>/<strong>stringr</strong> and regular expressions (see Appendix <a href="https://smltar.com/regexp.html#regexp">A</a>).</p>
<p>There are two main approaches to tokenization.</p>
<ol style="list-style-type: decimal">
<li><em>Split</em> the string up according to some rule.</li>
<li><em>Extract</em> tokens based on some rule.</li>
</ol>
<p>The number and complexity of our rules are determined by our desired outcome. We can reach complex outcomes by chaining together many smaller rules. In this section, we will implement a couple of specialty tokenizers to showcase these techniques.</p>
<div id="tokenize-to-characters-only-keeping-letters" class="section level3" number="2.4.1">
<h3 class="hasAnchor"><span class="header-section-number">2.4.1</span> Tokenize to characters, only keeping letters<a href="https://smltar.com/tokenization.html#tokenize-to-characters-only-keeping-letters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here we want to modify what <code>tokenize_characters()</code> does, such that we only keep letters. There are two main options. We can use <code>tokenize_characters()</code> and remove anything that is not a letter, or we can extract the letters one by one. Lets try the latter option. This is an <strong>extract</strong> task, and we will use <code>str_extract_all()</code> as each string has the possibility of including more than one token. Since we want to extract letters we can use the letters character class <code>[:alpha:]</code> to match letters and the quantifier <code>{1}</code> to only extract the first one.</p>
<div class="rmdnote">
<p>
In this example, leaving out the quantifier yields the same result as including it. However, for more complex regular expressions, specifying the quantifier allows the string handling to run faster.
</p>
</div>
<div class="sourceCode" id="cb41"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="https://smltar.com/tokenization.html#cb41-1" aria-hidden="true" tabindex="-1"></a>letter_tokens <span class="ot">&lt;-</span> <span class="fu">str_extract_all</span>(</span>
<span id="cb41-2"><a href="https://smltar.com/tokenization.html#cb41-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">string =</span> <span class="st">"This sentence include 2 numbers and 1 period."</span>,</span>
<span id="cb41-3"><a href="https://smltar.com/tokenization.html#cb41-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">pattern =</span> <span class="st">"[:alpha:]{1}"</span></span>
<span id="cb41-4"><a href="https://smltar.com/tokenization.html#cb41-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-5"><a href="https://smltar.com/tokenization.html#cb41-5" aria-hidden="true" tabindex="-1"></a>letter_tokens</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt;  [1] "T" "h" "i" "s" "s" "e" "n" "t" "e" "n" "c" "e" "i" "n" "c" "l" "u" "d" "e"
#&gt; [20] "n" "u" "m" "b" "e" "r" "s" "a" "n" "d" "p" "e" "r" "i" "o" "d"</code></pre>
<p>We may be tempted to specify the character class as something like <code>[a-zA-Z]{1}</code>. This option would run faster, but we would lose non-English letter characters. This is a design choice we have to make depending on the goals of our specific problem.</p>
<div class="sourceCode" id="cb43"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="https://smltar.com/tokenization.html#cb43-1" aria-hidden="true" tabindex="-1"></a>danish_sentence <span class="ot">&lt;-</span> <span class="st">"S mdte han en gammel heks p landevejen"</span></span>
<span id="cb43-2"><a href="https://smltar.com/tokenization.html#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="https://smltar.com/tokenization.html#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="fu">str_extract_all</span>(danish_sentence, <span class="st">"[:alpha:]"</span>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt;  [1] "S" "" "m" "" "d" "t" "e" "h" "a" "n" "e" "n" "g" "a" "m" "m" "e" "l" "h"
#&gt; [20] "e" "k" "s" "p" "" "l" "a" "n" "d" "e" "v" "e" "j" "e" "n"</code></pre>
<div class="sourceCode" id="cb45"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="https://smltar.com/tokenization.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str_extract_all</span>(danish_sentence, <span class="st">"[a-zA-Z]"</span>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt;  [1] "S" "m" "d" "t" "e" "h" "a" "n" "e" "n" "g" "a" "m" "m" "e" "l" "h" "e" "k"
#&gt; [20] "s" "p" "l" "a" "n" "d" "e" "v" "e" "j" "e" "n"</code></pre>
<div class="rmdwarning">
<p>
Choosing between <code>[:alpha:]</code> and <code>[a-zA-Z]</code> may seem quite similar, but the resulting differences can have a big impact on your analysis.
</p>
</div>
</div>
<div id="allow-for-hyphenated-words" class="section level3" number="2.4.2">
<h3 class="hasAnchor"><span class="header-section-number">2.4.2</span> Allow for hyphenated words<a href="https://smltar.com/tokenization.html#allow-for-hyphenated-words" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In our examples so far, we have noticed that the string fir-tree is typically split into two tokens. Lets explore two different approaches for how to handle this hyphenated word as one token. First, lets split on white space; this is a decent way to identify words in English and some other languages, and it does not split hyphenated words as the hyphen character isnt considered a white-space. Second, lets find a regex to match words with a hyphen and extract those.</p>
<p>Splitting by white space is not too difficult because we can use character classes, as shown in Table <a href="https://smltar.com/regexp.html#tab:characterclasses">A.2</a>. We will use the white space character class <code>[:space:]</code> to split our sentence.</p>
<div class="sourceCode" id="cb47"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="https://smltar.com/tokenization.html#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str_split</span>(<span class="st">"This isn't a sentence with hyphenated-words."</span>, <span class="st">"[:space:]"</span>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "This"              "isn't"             "a"                
#&gt; [4] "sentence"          "with"              "hyphenated-words."</code></pre>
<p>This worked pretty well. This version doesnt drop punctuation, but we can achieve this by removing punctuation characters at the beginning and end of words.</p>
<div class="sourceCode" id="cb49"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="https://smltar.com/tokenization.html#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str_split</span>(<span class="st">"This isn't a sentence with hyphenated-words."</span>, <span class="st">"[:space:]"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb49-2"><a href="https://smltar.com/tokenization.html#cb49-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(<span class="sc">~</span> <span class="fu">str_remove_all</span>(.x, <span class="st">"^[:punct:]+|[:punct:]+$"</span>))</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "This"             "isn't"            "a"                "sentence"        
#&gt; [5] "with"             "hyphenated-words"</code></pre>
<p>This regex used to remove the punctuation is a little complicated, so lets discuss it piece by piece.</p>
<ul>
<li><p>The regex <code>^[:punct:]+</code> will look at the beginning of the string (<code>^</code>) to match any punctuation characters (<code>[:punct:]</code>), where it will select one or more (<code>+</code>).</p></li>
<li><p>The other regex <code>[:punct:]+$</code> will look for punctuation characters (<code>[:punct:]</code>) that appear one or more times (<code>+</code>) at the end of the string (<code>$</code>).</p></li>
<li><p>These will alternate (<code>|</code>) so that we get matches from both sides of the words.</p></li>
<li><p>The reason we use the quantifier <code>+</code> is that there are cases where a word is followed by multiple characters we dont want, such as <code>"okay..."</code> and <code>"Really?!!!"</code>.</p></li>
</ul>
<p>We are using <code>map()</code> since <code>str_split()</code> returns a list, and we want <code>str_remove_all()</code> to be applied to each element in the list. (The example here only has one element.)</p>
<p>Now lets see if we can get the same result using extraction. We will start by constructing a regular expression that will capture hyphenated words; our definition here is a word with one hyphen located inside it. Since we want the hyphen to be inside the word, we will need to have a non-zero number of characters on either side of the hyphen.</p>
<div class="sourceCode" id="cb51"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="https://smltar.com/tokenization.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str_extract_all</span>(</span>
<span id="cb51-2"><a href="https://smltar.com/tokenization.html#cb51-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">string =</span> <span class="st">"This isn't a sentence with hyphenated-words."</span>,</span>
<span id="cb51-3"><a href="https://smltar.com/tokenization.html#cb51-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">pattern =</span> <span class="st">"[:alpha:]+-[:alpha:]+"</span></span>
<span id="cb51-4"><a href="https://smltar.com/tokenization.html#cb51-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "hyphenated-words"</code></pre>
<p>Wait, this only matched the hyphenated word! This happened because we are only matching words with hyphens. If we add the quantifier <code>?</code> then we can match 0 or 1 occurrences.</p>
<div class="sourceCode" id="cb53"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="https://smltar.com/tokenization.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str_extract_all</span>(</span>
<span id="cb53-2"><a href="https://smltar.com/tokenization.html#cb53-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">string =</span> <span class="st">"This isn't a sentence with hyphenated-words."</span>,</span>
<span id="cb53-3"><a href="https://smltar.com/tokenization.html#cb53-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">pattern =</span> <span class="st">"[:alpha:]+-?[:alpha:]+"</span></span>
<span id="cb53-4"><a href="https://smltar.com/tokenization.html#cb53-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "This"             "isn"              "sentence"         "with"            
#&gt; [5] "hyphenated-words"</code></pre>
<p>Now we are getting more words, but the ending of <code>"isn't"</code> is not there anymore and we lost the word <code>"a"</code>. We can get matches for the whole contraction by expanding the character class <code>[:alpha:]</code> to include the character <code>'</code>. We do that by using <code>[[:alpha:]']</code>.</p>
<div class="sourceCode" id="cb55"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="https://smltar.com/tokenization.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str_extract_all</span>(</span>
<span id="cb55-2"><a href="https://smltar.com/tokenization.html#cb55-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">string =</span> <span class="st">"This isn't a sentence with hyphenated-words."</span>,</span>
<span id="cb55-3"><a href="https://smltar.com/tokenization.html#cb55-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">pattern =</span> <span class="st">"[[:alpha:]']+-?[[:alpha:]']+"</span></span>
<span id="cb55-4"><a href="https://smltar.com/tokenization.html#cb55-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "This"             "isn't"            "sentence"         "with"            
#&gt; [5] "hyphenated-words"</code></pre>
<p>Next, we need to find out why <code>"a"</code> wasnt matched. If we look at the regular expression, we remember that we imposed the restriction that a non-zero number of characters needed to surround the hyphen to avoid matching words that start or end with a hyphen. This means that the smallest possible pattern matched is two characters long. We can fix this by using an alternation with <code>|</code>. We will keep our previous match on the left-hand side, and include <code>[:alpha:]{1}</code> on the right-hand side to match the single length words that wont be picked up by the left-hand side. Notice how we arent using <code>[[:alpha:]']</code> since we are not interested in matching single <code>'</code> characters.</p>
<div class="sourceCode" id="cb57"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="https://smltar.com/tokenization.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str_extract_all</span>(</span>
<span id="cb57-2"><a href="https://smltar.com/tokenization.html#cb57-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">string =</span> <span class="st">"This isn't a sentence with hyphenated-words."</span>,</span>
<span id="cb57-3"><a href="https://smltar.com/tokenization.html#cb57-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">pattern =</span> <span class="st">"[[:alpha:]']+-?[[:alpha:]']+|[:alpha:]{1}"</span></span>
<span id="cb57-4"><a href="https://smltar.com/tokenization.html#cb57-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "This"             "isn't"            "a"                "sentence"        
#&gt; [5] "with"             "hyphenated-words"</code></pre>
<p>That is getting to be quite a complex regex, but we are now getting the same answer as before.</p>
</div>
<div id="wrapping-it-in-a-function" class="section level3" number="2.4.3">
<h3 class="hasAnchor"><span class="header-section-number">2.4.3</span> Wrapping it in a function<a href="https://smltar.com/tokenization.html#wrapping-it-in-a-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have shown how we can use regular expressions to extract the tokens we want, perhaps to use in modeling. So far, the code has been rather unstructured. We would ideally wrap these tasks into functions that can be used the same way <code>tokenize_words()</code> is used.</p>
<p>Lets start with the example with hyphenated words. To make the function a little more flexible, lets add an option to transform all the output to lowercase.</p>
<div class="sourceCode" id="cb59"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="https://smltar.com/tokenization.html#cb59-1" aria-hidden="true" tabindex="-1"></a>tokenize_hyphenated_words <span class="ot">&lt;-</span> <span class="cf">function</span>(x, <span class="at">lowercase =</span> <span class="cn">TRUE</span>) {</span>
<span id="cb59-2"><a href="https://smltar.com/tokenization.html#cb59-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (lowercase)</span>
<span id="cb59-3"><a href="https://smltar.com/tokenization.html#cb59-3" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">str_to_lower</span>(x)</span>
<span id="cb59-4"><a href="https://smltar.com/tokenization.html#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="https://smltar.com/tokenization.html#cb59-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">str_split</span>(x, <span class="st">"[:space:]"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb59-6"><a href="https://smltar.com/tokenization.html#cb59-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">map</span>(<span class="sc">~</span> <span class="fu">str_remove_all</span>(.x, <span class="st">"^[:punct:]+|[:punct:]+$"</span>))</span>
<span id="cb59-7"><a href="https://smltar.com/tokenization.html#cb59-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb59-8"><a href="https://smltar.com/tokenization.html#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="https://smltar.com/tokenization.html#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenize_hyphenated_words</span>(the_fir_tree[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>])</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt;  [1] "far"    "down"   "in"     "the"    "forest" "where"  "the"    "warm"  
#&gt;  [9] "sun"    "and"    "the"    "fresh"  "air"    "made"   "a"      "sweet" 
#&gt; 
#&gt; [[2]]
#&gt;  [1] "resting-place" "grew"          "a"             "pretty"       
#&gt;  [5] "little"        "fir-tree"      "and"           "yet"          
#&gt;  [9] "it"            "was"           "not"           "happy"        
#&gt; [13] "it"           
#&gt; 
#&gt; [[3]]
#&gt;  [1] "wished"     "so"         "much"       "to"         "be"        
#&gt;  [6] "tall"       "like"       "its"        "companions" "the"       
#&gt; [11] "pines"      "and"        "firs"       "which"      "grew"</code></pre>
<p>Notice how we transformed to lowercase first because the rest of the operations are case insensitive.</p>
<p>Next lets turn our character n-gram tokenizer into a function, with a variable <code>n</code> argument.</p>
<div class="sourceCode" id="cb61"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="https://smltar.com/tokenization.html#cb61-1" aria-hidden="true" tabindex="-1"></a>tokenize_character_ngram <span class="ot">&lt;-</span> <span class="cf">function</span>(x, n) {</span>
<span id="cb61-2"><a href="https://smltar.com/tokenization.html#cb61-2" aria-hidden="true" tabindex="-1"></a>  ngram_loc <span class="ot">&lt;-</span> <span class="fu">str_locate_all</span>(x, <span class="fu">paste0</span>(<span class="st">"(?=(</span><span class="sc">\\</span><span class="st">w{"</span>, n, <span class="st">"}))"</span>))</span>
<span id="cb61-3"><a href="https://smltar.com/tokenization.html#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="https://smltar.com/tokenization.html#cb61-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map2</span>(ngram_loc, x, <span class="sc">~</span><span class="fu">str_sub</span>(.y, .x[, <span class="dv">1</span>], .x[, <span class="dv">1</span>] <span class="sc">+</span> n <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb61-5"><a href="https://smltar.com/tokenization.html#cb61-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb61-6"><a href="https://smltar.com/tokenization.html#cb61-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-7"><a href="https://smltar.com/tokenization.html#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenize_character_ngram</span>(the_fir_tree[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], <span class="at">n =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt;  [1] "Far" "dow" "own" "the" "for" "ore" "res" "est" "whe" "her" "ere" "the"
#&gt; [13] "war" "arm" "sun" "and" "the" "fre" "res" "esh" "air" "mad" "ade" "swe"
#&gt; [25] "wee" "eet"
#&gt; 
#&gt; [[2]]
#&gt;  [1] "res" "est" "sti" "tin" "ing" "pla" "lac" "ace" "gre" "rew" "pre" "ret"
#&gt; [13] "ett" "tty" "lit" "itt" "ttl" "tle" "fir" "tre" "ree" "and" "yet" "was"
#&gt; [25] "not" "hap" "app" "ppy"
#&gt; 
#&gt; [[3]]
#&gt;  [1] "wis" "ish" "she" "hed" "muc" "uch" "tal" "all" "lik" "ike" "its" "com"
#&gt; [13] "omp" "mpa" "pan" "ani" "nio" "ion" "ons" "the" "pin" "ine" "nes" "and"
#&gt; [25] "fir" "irs" "whi" "hic" "ich" "gre" "rew"</code></pre>
<p>We can use <code>paste0()</code> in this function to construct an actual regex.</p>
</div>
</div>
<div id="tokenization-for-non-latin-alphabets" class="section level2" number="2.5">
<h2 class="hasAnchor"><span class="header-section-number">2.5</span> Tokenization for non-Latin alphabets<a href="https://smltar.com/tokenization.html#tokenization-for-non-latin-alphabets" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our discussion of tokenization so far has focused on text where words are separated by white space and punctuation. For such text, even a quite basic tokenizer can give decent results. However, many written languages dont separate words in this way.</p>
<p>One of these languages is Chinese where each word can be represented by one or more consecutive characters.
Splitting Chinese text into words is called word segmentation and is still an active area of research <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-ma-etal-2018-state" role="doc-biblioref">Ma, Ganchev, and Weiss 2018</a>; <a href="https://smltar.com/tokenization.html#ref-Huang2019" role="doc-biblioref">Huang et al. 2020</a>)</span>.</p>
<p>We are not going to go into depth in this area, but we want to showcase that word segmentation is indeed possible with R as well. We use the <strong>jiebaR</strong> package <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-R-jiebaR" role="doc-biblioref">Wenfeng and Yanyi 2019</a>)</span>. It is conceptually similar to the <strong>tokenizers</strong> package, but we need to create a worker that is passed into <code>segment()</code> along with the string we want to segment.</p>
<div class="sourceCode" id="cb63"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="https://smltar.com/tokenization.html#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(jiebaR)</span>
<span id="cb63-2"><a href="https://smltar.com/tokenization.html#cb63-2" aria-hidden="true" tabindex="-1"></a>words <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">""</span>, <span class="st">""</span>)</span>
<span id="cb63-3"><a href="https://smltar.com/tokenization.html#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="https://smltar.com/tokenization.html#cb63-4" aria-hidden="true" tabindex="-1"></a>engine1 <span class="ot">&lt;-</span> <span class="fu">worker</span>(<span class="at">bylines =</span> <span class="cn">TRUE</span>)</span>
<span id="cb63-5"><a href="https://smltar.com/tokenization.html#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="https://smltar.com/tokenization.html#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="fu">segment</span>(words, engine1)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] "" ""   ""   "" "" ""   ""
#&gt; 
#&gt; [[2]]
#&gt; [1] "" ""   ""   "" ""   ""</code></pre>
</div>
<div id="tokenization-benchmark" class="section level2" number="2.6">
<h2 class="hasAnchor"><span class="header-section-number">2.6</span> Tokenization benchmark<a href="https://smltar.com/tokenization.html#tokenization-benchmark" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Not all tokenization packages are the same. Most open-source tokenizers in R are well-designed, but they are designed to serve different purposes. Some have a multitude of arguments to allow you to customize your tokenizer for greater flexibility, but this flexibility comes at a price; they tend to have relatively slower performance.</p>
<p>While we cant easily quantify flexibility, it is straightforward to benchmark some of the tokenizers available in R so you can pick the one that best suits your needs.</p>
<div class="sourceCode" id="cb65"><button type="button" class="copy-to-clipboard-button" title="Copy to clipboard" aria-label="Copy to clipboard"><i class="fa fa-copy"></i></button><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="https://smltar.com/tokenization.html#cb65-1" aria-hidden="true" tabindex="-1"></a>bench<span class="sc">::</span><span class="fu">mark</span>(<span class="at">check =</span> <span class="cn">FALSE</span>, <span class="at">iterations =</span> <span class="dv">10</span>,</span>
<span id="cb65-2"><a href="https://smltar.com/tokenization.html#cb65-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">`</span><span class="at">corpus</span><span class="st">`</span> <span class="ot">=</span> corpus<span class="sc">::</span><span class="fu">text_tokens</span>(hcandersen_en<span class="sc">$</span>text),</span>
<span id="cb65-3"><a href="https://smltar.com/tokenization.html#cb65-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">`</span><span class="at">tokenizers</span><span class="st">`</span> <span class="ot">=</span> tokenizers<span class="sc">::</span><span class="fu">tokenize_words</span>(hcandersen_en<span class="sc">$</span>text),</span>
<span id="cb65-4"><a href="https://smltar.com/tokenization.html#cb65-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">`</span><span class="at">text2vec</span><span class="st">`</span> <span class="ot">=</span> text2vec<span class="sc">::</span><span class="fu">word_tokenizer</span>(hcandersen_en<span class="sc">$</span>text),</span>
<span id="cb65-5"><a href="https://smltar.com/tokenization.html#cb65-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">`</span><span class="at">quanteda</span><span class="st">`</span> <span class="ot">=</span> quanteda<span class="sc">::</span><span class="fu">tokenize_word</span>(hcandersen_en<span class="sc">$</span>text),</span>
<span id="cb65-6"><a href="https://smltar.com/tokenization.html#cb65-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">`</span><span class="at">base R</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">strsplit</span>(hcandersen_en<span class="sc">$</span>text, <span class="st">"</span><span class="sc">\\</span><span class="st">s"</span>)</span>
<span id="cb65-7"><a href="https://smltar.com/tokenization.html#cb65-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 5  6
#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`
#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;
#&gt; 1 corpus       75.9ms     81ms     11.9     4.58MB     1.33
#&gt; 2 tokenizers   96.2ms    104ms      9.54    1.01MB     2.38
#&gt; 3 text2vec     78.5ms     82ms     11.9    15.61MB     1.33
#&gt; 4 quanteda    163.2ms    172ms      5.69     8.7MB     1.42
#&gt; 5 base R      294.1ms    302ms      3.15   10.51MB     2.10</code></pre>
<p>The corpus package <span class="citation">(<a href="https://smltar.com/tokenization.html#ref-Perry2020" role="doc-biblioref">Perry 2020</a>)</span> offers excellent performance for tokenization, and other options are not much worse. One exception is using a base R function as a tokenizer; you will see significant performance gains by instead using a package built specifically for text tokenization.</p>
</div>
<div id="tokensummary" class="section level2" number="2.7">
<h2 class="hasAnchor"><span class="header-section-number">2.7</span> Summary<a href="https://smltar.com/tokenization.html#tokensummary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To build a predictive model, text data needs to be split into meaningful units, called tokens. These tokens range from individual characters to words to n-grams and even more complex structures, and the particular procedure used to identify tokens from text can be important to your results. Fast and consistent tokenizers are available, but understanding how they behave and in what circumstances they work best will set you up for success. Its also possible to build custom tokenizers when necessary. Once text data is tokenized, a common next preprocessing step is to consider how to handle very common words that are not very informative stop words. Chapter <a href="https://smltar.com/stopwords.html#stopwords">3</a> examines this in detail.</p>
<div id="in-this-chapter-you-learned-1" class="section level3" number="2.7.1">
<h3 class="hasAnchor"><span class="header-section-number">2.7.1</span> In this chapter, you learned:<a href="https://smltar.com/tokenization.html#in-this-chapter-you-learned-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>that tokens are meaningful units of text, such as words or n-grams</p></li>
<li><p>to implement different kinds of tokenization, the process of splitting text into tokens</p></li>
<li><p>how different kinds of tokenization affect the distribution of tokens</p></li>
<li><p>how to build your own tokenizer when the fast, consistent tokenizers that are available are not flexible enough</p></li>
</ul>

</div>
</div>
</div>
<h3>References<a href="https://smltar.com/references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Bender13" class="csl-entry">
Bender, E. M. 2013. <span>Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax.</span> <em>Synthesis Lectures on Human Language Technologies</em> 6 (3). Morgan &amp; Claypool Publishers: 1184.
</div>
<div id="ref-Gagolewski19" class="csl-entry">
Gagolewski, M. 2020. <em><span class="nocase">stringi</span>: Character String Processing Facilities</em>. R package version 1.6.2. <a href="http://www.gagolewski.com/software/stringi/">http://www.gagolewski.com/software/stringi/</a>.
</div>
<div id="ref-spacy2" class="csl-entry">
Honnibal, M., Montani, I., Van Landeghem, S., and Boyd, A. 2020. <em><span class="nocase">spaCy: Industrial-strength Natural Language Processing in Python</span></em>. Zenodo. <a href="https://doi.org/10.5281/zenodo.1212303">https://doi.org/10.5281/zenodo.1212303</a>.
</div>
<div id="ref-Huang2019" class="csl-entry">
Huang, W., Cheng, X., Chen, K., Wang, T., and Chu, W. 2020. <span>Towards Fast and Accurate Neural <span>C</span>hinese Word Segmentation with Multi-Criteria Learning.</span> In <em>Proceedings of the 28th International Conference on Computational Linguistics</em>, 20622072. Barcelona, Spain (Online): International Committee on Computational Linguistics. <a href="https://www.aclweb.org/anthology/2020.coling-main.186">https://www.aclweb.org/anthology/2020.coling-main.186</a>.
</div>
<div id="ref-R-hcandersenr" class="csl-entry">
Hvitfeldt, E. 2019a. <em><span class="nocase">hcandersenr</span>: <span class="nocase">H.C. Andersens</span> Fairy Tales</em>. R package version 0.2.0. <a href="https://cran.r-project.org/package=hcandersenr">https://CRAN.R-project.org/package=hcandersenr</a>.
</div>
<div id="ref-ma-etal-2018-state" class="csl-entry">
Ma, J., Ganchev, K., and Weiss, D. 2018. <span>State-of-the-Art <span>C</span>hinese Word Segmentation with Bi-<span>LSTM</span>s.</span> In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, 49024908. Brussels, Belgium: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/D18-1529">https://www.aclweb.org/anthology/D18-1529</a>.
</div>
<div id="ref-Manning:2008:IIR:1394399" class="csl-entry">
Manning, C. D., Raghavan, P., and Schtze, H. 2008. <em>Introduction to Information Retrieval</em>. New York, NY: Cambridge University Press.
</div>
<div id="ref-Mullen18" class="csl-entry">
Mullen, L. A., Benoit, K., Keyes, O., Selivanov, D., and Arnold, J. 2018. <span>Fast, Consistent Tokenization of Natural Language Text.</span> <em>Journal of Open Source Software</em> 3: 655. <a href="https://doi.org/10.21105/joss.00655">https://doi.org/10.21105/joss.00655</a>.
</div>
<div id="ref-Perry2020" class="csl-entry">
Perry, P. O. 2020. <em><span class="nocase">corpus</span>: Text Corpus Analysis</em>. R package version 0.10.2. <a href="https://cran.r-project.org/package=corpus">https://CRAN.R-project.org/package=corpus</a>.
</div>
<div id="ref-Silge16" class="csl-entry">
Silge, J., and Robinson, D. 2016. <span>Tidytext: Text Mining and Analysis Using Tidy Data Principles in <span>R</span>.</span> <em>JOSS</em> 1 (3). The Open Journal. <a href="http://dx.doi.org/10.21105/joss.00037">http://dx.doi.org/10.21105/joss.00037</a>.
</div>
<div id="ref-R-jiebaR" class="csl-entry">
Wenfeng, Q., and Yanyi, W. 2019. <em><span class="nocase">jiebaR</span>: Chinese Text Segmentation</em>. R package version 0.11. <a href="https://cran.r-project.org/package=jiebaR">https://CRAN.R-project.org/package=jiebaR</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="https://smltar.com/language.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="https://smltar.com/stopwords.html" class="navigation navigation-next " aria-label="Next page" style="margin-right: 0px;"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/app.min.js.download"></script>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/clipboard.min.js.download"></script>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-search.js.download"></script>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-sharing.js.download"></script>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-fontsettings.js.download"></script>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-bookdown.js.download"></script>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/jquery.highlight.js.download"></script>
<script src="./Chapter 2 Tokenization _ Supervised Machine Learning for Text Analysis in R_files/plugin-clipboard.js.download"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": null,
"edit": {
"link": "https://github.com/EmilHvitfeldt/smltar/edit/master/02_tokenization.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>



</body></html>